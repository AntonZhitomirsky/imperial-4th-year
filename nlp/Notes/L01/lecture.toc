\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Dealing with natural language}{2}{section.2}%
\contentsline {subsection}{\numberline {1.1}Complexities}{2}{subsection.3}%
\contentsline {subsection}{\numberline {1.2}The role of Deep Learning}{2}{subsection.4}%
\contentsline {subsection}{\numberline {1.3}Composites of Language}{2}{subsection.5}%
\contentsline {subsubsection}{\numberline {1.3.1}Lexicon - morphological analysis}{2}{subsubsection.6}%
\contentsline {subsubsection}{\numberline {1.3.2}Syntax}{3}{subsubsection.15}%
\contentsline {subsubsection}{\numberline {1.3.3}Semantics}{4}{subsubsection.16}%
\contentsline {subsubsection}{\numberline {1.3.4}Discourse}{4}{subsubsection.17}%
\contentsline {subsubsection}{\numberline {1.3.5}Pragmatics}{4}{subsubsection.18}%
\contentsline {section}{\numberline {2}How to represent language to an algorithm}{4}{section.19}%
\contentsline {subsection}{\numberline {2.1}One-hot-encoding}{4}{subsection.20}%
\contentsline {section}{\numberline {3}ML Refresher}{5}{section.21}%
\contentsline {subsection}{\numberline {3.1}Linear activation function}{5}{subsection.22}%
\contentsline {subsection}{\numberline {3.2}Non-linear activation functions}{5}{subsection.25}%
\contentsline {subsubsection}{\numberline {3.2.1}Sigmoid}{5}{subsubsection.26}%
\contentsline {subsubsection}{\numberline {3.2.2}ReLU}{6}{subsubsection.29}%
\contentsline {subsubsection}{\numberline {3.2.3}Tanh}{7}{subsubsection.32}%
\contentsline {subsubsection}{\numberline {3.2.4}Softmax}{8}{subsubsection.35}%
\contentsline {subsection}{\numberline {3.3}Loss Functions}{8}{subsection.39}%
\contentsline {subsubsection}{\numberline {3.3.1}Mean suqared error}{8}{subsubsection.40}%
\contentsline {subsubsection}{\numberline {3.3.2}Binary cross-entropy}{9}{subsubsection.42}%
\contentsline {subsubsection}{\numberline {3.3.3}Categorical cross-entropy}{9}{subsubsection.44}%
\contentsline {subsection}{\numberline {3.4}Regularization}{9}{subsection.46}%
