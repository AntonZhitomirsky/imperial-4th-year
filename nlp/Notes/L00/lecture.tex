\documentclass[11pt]{article}

% some definitions for the title page
\newcommand{\reporttitle}{Intro Lecture and foundations}
\newcommand{\reportdescription}{}

% load some definitions and default packages
\input{../../../.latex-templates/includes}
\input{../../../.latex-templates/notation}

\usepackage{catchfilebetweentags} % to read input from another file

\bibliography{../bibliography}

\begin{document}

% Include the title page
\input{../../../.latex-templates/titlepage}

\tableofcontents

\clearpage

\section{Pytorch}

Pytorch will be used for the coursework, recommended is to look at \href{https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html}{this} link.

\section{Books}

Recommended NLP Books:

\begin{itemize}
    \item Speech and Language Processing. Dan Jurafsky and James H. Martin~\cite{book-speech-and-language-processing}
    \item A Primer on Neural Network Models for Natural Language Processing.
          Yoav Goldberg~\cite{primer-on-nlp}
    \item Natural Language Processing. Jacob Eisenstein~\cite{git-natural-language-processing}
\end{itemize}

Recommended ML Books:

\begin{itemize}
    \item Artificial Intelligence: a Modern Approach. (2009) Stuart Russell \& Peter Norvig~\cite{AI-modern-approach} with the solutions available at~\cite{AI-modern-approach-slutions}
    \item Machine Learning. (1997) Tom Mitchell~\cite{tom-mitchell-book}
    \item Neural Networks and Deep Learning. Michael A. Nielsen.
    \item Introduction to Deep Learning. Eugene Charniak~\cite{intro-to-dl-eugene-charniak}
    \item Deep Learning by Ian Goodfellow~\cite{Goodfellow-et-al-2016}
\end{itemize}

State of the art NLP:

\begin{itemize}
    \item Papers with code:~\cite{papers-with-code}
    \item  track the progress in Natural Language Processing:~\cite{track-progress-nlp}
\end{itemize}

\section{ML Refresher}

\subsection{Linear activation function}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-linear}

\subsection{Non-linear activation functions}

\subsubsection{Sigmoid}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-sigmoid}

\subsubsection{ReLU}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-relu}

\subsubsection{Tanh}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-tanh}

\subsubsection{Softmax}

\begin{figure}[H]
    \centering
    \includegraphics*[width=.3\linewidth]{figures/softmax.png}
\end{figure}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-softmax}

\subsection{Loss Functions}

\subsubsection{Mean suqared error}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-mse}

\subsubsection{Binary cross-entropy}

\ExecuteMetaData[../../../deep-learning/Notes/L03 - Activation and Loss/lecture]{nlp-bce}

\subsubsection{Categorical cross-entropy}

useful for multi-label classification (predicting one class out of many)

\begin{equation}
    L = - \frac 1 N \sum^N_{i=1}\sum^C_{c=1} y_c^{(i)}\log(\hat{y}_c^{(i)})
\end{equation}

\subsection{Regularization}

\begin{quote}
    any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. (See Chapter 7 of~\cite{Goodfellow-et-al-2016})
\end{quote}

\section{Multi-Layer Perceptron Math}

\begin{warning}
    Section not complete. Content of L01.1
\end{warning}

\printbibliography

\end{document}