\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Activation Functions}{3}{section.2}%
\contentsline {subsubsection}{\numberline {1.0.1}Introduction}{3}{subsubsection.3}%
\contentsline {subsubsection}{\numberline {1.0.2}Bounded Value Clipping}{3}{subsubsection.4}%
\contentsline {subsection}{\numberline {1.1}Linear}{3}{subsection.5}%
\contentsline {subsection}{\numberline {1.2}Non-linear Activation Functions}{4}{subsection.8}%
\contentsline {subsubsection}{\numberline {1.2.1}Sigmoid}{4}{subsubsection.9}%
\contentsline {subsubsection}{\numberline {1.2.2}Tanh}{5}{subsubsection.12}%
\contentsline {subsubsection}{\numberline {1.2.3}ReLU}{6}{subsubsection.15}%
\contentsline {subsubsection}{\numberline {1.2.4}Leaky ReLU}{6}{subsubsection.18}%
\contentsline {subsubsection}{\numberline {1.2.5}PReLU}{7}{subsubsection.21}%
\contentsline {subsubsection}{\numberline {1.2.6}SoftPlus}{8}{subsubsection.24}%
\contentsline {subsubsection}{\numberline {1.2.7}Exponential Linear Unit}{8}{subsubsection.27}%
\contentsline {subsubsection}{\numberline {1.2.8}Continuously Differentiable Exponential Linear Unit}{9}{subsubsection.30}%
\contentsline {subsubsection}{\numberline {1.2.9}Scaled Exponential Linear Unit}{10}{subsubsection.33}%
\contentsline {subsubsection}{\numberline {1.2.10}Gaussian Error Linear Unit}{11}{subsubsection.36}%
\contentsline {subsubsection}{\numberline {1.2.11}ReLU6}{11}{subsubsection.39}%
\contentsline {subsubsection}{\numberline {1.2.12}LogSigmoid}{12}{subsubsection.42}%
\contentsline {subsubsection}{\numberline {1.2.13}Softmin}{12}{subsubsection.45}%
\contentsline {subsubsection}{\numberline {1.2.14}Softmax}{13}{subsubsection.48}%
\contentsline {subsubsection}{\numberline {1.2.15}LogSoftmax}{13}{subsubsection.51}%
\contentsline {subsection}{\numberline {1.3}Period activations}{14}{subsection.54}%
\contentsline {subsubsection}{\numberline {1.3.1}Sinusoidal Representation Network}{14}{subsubsection.55}%
\contentsline {subsection}{\numberline {1.4}Summary}{15}{subsection.57}%
\contentsline {section}{\numberline {2}Loss}{15}{section.58}%
\contentsline {subsection}{\numberline {2.1}L2 Norm}{15}{subsection.59}%
\contentsline {subsection}{\numberline {2.2}L1 Norm}{15}{subsection.61}%
\contentsline {subsection}{\numberline {2.3}Smooth L1}{16}{subsection.63}%
\contentsline {subsection}{\numberline {2.4}Negative log likelihood loss}{16}{subsection.65}%
\contentsline {subsection}{\numberline {2.5}Cross Entropy (CE) Loss}{16}{subsection.67}%
\contentsline {subsection}{\numberline {2.6}Binary Cross Entropy (BCE) Loss}{17}{subsection.69}%
\contentsline {subsection}{\numberline {2.7}Kullback-Leibler Divergence Loss}{17}{subsection.71}%
\contentsline {subsection}{\numberline {2.8}Margin Ranking Loss/Ranking Losses/Contrastive Loss}{18}{subsection.73}%
\contentsline {subsection}{\numberline {2.9}Triplet Margin Loss}{18}{subsection.75}%
\contentsline {subsection}{\numberline {2.10}Cosine Embedding Loss}{19}{subsection.77}%
\contentsline {subsection}{\numberline {2.11}Summary}{19}{subsection.79}%
