\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Activation Functions}{3}{section.2}%
\contentsline {subsection}{\numberline {1.1}Linear}{3}{subsection.3}%
\contentsline {subsection}{\numberline {1.2}Non-linear Activation Functions}{4}{subsection.6}%
\contentsline {subsubsection}{\numberline {1.2.1}Sigmoid}{4}{subsubsection.7}%
\contentsline {subsubsection}{\numberline {1.2.2}Tanh}{5}{subsubsection.10}%
\contentsline {subsubsection}{\numberline {1.2.3}ReLU}{6}{subsubsection.13}%
\contentsline {subsubsection}{\numberline {1.2.4}Leaky ReLU}{6}{subsubsection.16}%
\contentsline {subsubsection}{\numberline {1.2.5}PReLU}{7}{subsubsection.19}%
\contentsline {subsubsection}{\numberline {1.2.6}SoftPlus}{8}{subsubsection.22}%
\contentsline {subsubsection}{\numberline {1.2.7}Exponential Linear Unit}{8}{subsubsection.25}%
\contentsline {subsubsection}{\numberline {1.2.8}Continuously Differentiable Exponential Linear Unit}{9}{subsubsection.28}%
\contentsline {subsubsection}{\numberline {1.2.9}Scaled Exponential Linear Unit}{10}{subsubsection.31}%
\contentsline {subsubsection}{\numberline {1.2.10}Gaussian Error Linear Unit}{11}{subsubsection.34}%
\contentsline {subsubsection}{\numberline {1.2.11}ReLU6}{11}{subsubsection.37}%
\contentsline {subsubsection}{\numberline {1.2.12}LogSigmoid}{12}{subsubsection.40}%
\contentsline {subsubsection}{\numberline {1.2.13}Softmin}{12}{subsubsection.43}%
\contentsline {subsubsection}{\numberline {1.2.14}Softmax}{13}{subsubsection.46}%
\contentsline {subsubsection}{\numberline {1.2.15}LogSoftmax}{13}{subsubsection.49}%
\contentsline {subsection}{\numberline {1.3}Period activations}{14}{subsection.52}%
\contentsline {subsubsection}{\numberline {1.3.1}Sinusoidal Representation Network}{14}{subsubsection.53}%
\contentsline {subsection}{\numberline {1.4}Summary}{15}{subsection.55}%
\contentsline {section}{\numberline {2}Loss}{15}{section.56}%
\contentsline {subsection}{\numberline {2.1}L2 Norm}{15}{subsection.57}%
\contentsline {subsection}{\numberline {2.2}L1 Norm}{15}{subsection.59}%
\contentsline {subsection}{\numberline {2.3}Smooth L1}{16}{subsection.61}%
\contentsline {subsection}{\numberline {2.4}Negative log likelihood loss}{16}{subsection.63}%
\contentsline {subsection}{\numberline {2.5}Cross Entropy (CE) Loss}{16}{subsection.65}%
\contentsline {subsection}{\numberline {2.6}Binary Cross Entropy (BCE) Loss}{17}{subsection.67}%
\contentsline {subsection}{\numberline {2.7}Kullback-Leibler Divergence Loss}{17}{subsection.69}%
\contentsline {subsection}{\numberline {2.8}Margin Ranking Loss/Ranking Losses/Contrastive Loss}{18}{subsection.71}%
\contentsline {subsection}{\numberline {2.9}Triplet Margin Loss}{18}{subsection.73}%
\contentsline {subsection}{\numberline {2.10}Cosine Embedding Loss}{19}{subsection.75}%
\contentsline {subsection}{\numberline {2.11}Summary}{19}{subsection.77}%
