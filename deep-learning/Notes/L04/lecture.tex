\documentclass[11pt]{article}

% some definitions for the title page
\newcommand{\reporttitle}{Popular Network Architectures (\& BatchNorm)}
\newcommand{\reportdescription}{LeNet-5~\cite{LeNet}, MNIST, AlexNet~\cite{AlexNet}, ImageNet, VGG~\cite{VGG}, Inception (GoogleLeNet), BatchNorm~\cite{BatchNorm}, ResNet~\cite{ResNet}, DenseNet, Squeeze-Excite~\cite{SqueezeNet} Net U-Net, Data Augmentation}

% load some definitions and default packages
\input{../../../.latex-templates/includes}
\input{../../../.latex-templates/notation}

\bibliography{bibliography}

\begin{document}

% Include the title page
\input{../../../.latex-templates/titlepage}

\tableofcontents

\clearpage

\section{Architectures}

\subsection{LeNet-5}

LeNet~\cite{LeNet} was initially designed for low-resolution, black and white image recognition, specifically for digits. It demonstrated that CNNs could reliably perform both tasks of object localization and recognition (on low-resolution black and white images).

The last steps of the LeNet-5 architecture employ fully connected layers to convert features into final predictions. The scenario of MNIST data didn't matter, because of its small number of output classes. This \textit{complicates the architecture when scaling up, influencing both computational resources and architecture}.

The Convolutional Neural Network Architecture ensures some degree of shift, scale and distortion invariance: \textit{local receptive fields, shared weights} (or weight replication), and spatial or temporal \textit{sub-sampling}.

Once a feature has been detected, its exact location
becomes less important. Only its approximate position
relative to other features is relevant.

\subsubsection{shared weights}

This algorithm is particularly useful for shared-weight networks because the weight sharing creates ill-conditioning of the error surface. Because of the sharing, one single parameter in the first few layers can have an enormous influence on the output. Consequently, the second derivative of the error with resp ect to this parameter maybe very large, while it can be quite small for other parameters elsewhere in the network

\subsubsection{sub-sampling}

A simple way to reduce the precision with which the position of distinctive features are encoded in a feature map is to reduce the spatial resolution of the feature map. This can be achieved with a so-called sub-sampling layers which performs a local \textbf{averaging} and a sub-sampling, reducing the resolution of the feature map, and reducing the sensitivity of the output to shifts and distortions.

\subsubsection{Loss}

Maximum Likelihood Estimation Criterion (MLE), which is equivalent to the Mean Squared Error (MSE)

\begin{equation*}
    E(W) = \frac 1 P \sum ^ P _{ p = 1} y_{D^P} (Z^P, W)
\end{equation*}

Where $y_{D^P}$ is the output of the $D_p$-th RBF unit, i.e. the one that corresponds to the correct class of the input pattern $Z^p$. 

% It lacks three important properties:

% \begin{enumerate}
%     \item \textbf{Trivial Solution with Adaptation of RBF Parameters}:
%         If the parameters of the Radial Basis Function (RBF) are allowed to adapt, the MLE criterion has a trivial and unacceptable solution.
%         In this solution, all RBF parameter vectors become equal, leading to a constant and unchanging state of the network.
%         The network effectively ignores the input, and all RBF outputs become equal to zero.
%     \item \textbf{Lack of Competition Between Classes}:
%         The MLE criterion lacks competition between different classes in the training process.
%         Introducing a more discriminative training criterion, referred to as the Maximum A Posteriori (MAP) criterion, could address this issue.
%         The MAP criterion aims to maximize the posterior probability of the correct class or minimize the logarithm of the probability of the correct class given the input.
%         Unlike MLE, MAP introduces a competitive element by pushing up the penalties of incorrect classes in addition to pushing down the penalty of the correct class.
%     \item \textbf{Potential Collapsing Phenomenon}:
%         When RBF weights are allowed to adapt with the MLE criterion, there is a risk of a collapsing phenomenon where all RBF centers become equal.
%         The discriminative criterion of MAP prevents this collapsing effect by keeping the RBF centers apart from each other.
%         The discriminative criterion ensures that the posterior probabilities of different classes remain distinct, preventing the network from ignoring the input.
% \end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/LeNetFigure.png}
    \caption{Architecture of LeNet-5 a Convolutional Neural Network, here for digits recognition. Each plane is a feature map, i.e. a set of units
    whose weights are constrained to be identical~\cite{LeNet}}
\end{figure}

\lstinputlisting[language=python,firstline=1,lastline=38]{code/LetNet.py}


\subsection{AlexNet}

The AlexNet~\cite{AlexNet} was introduced in 2012 and introduced more layers for larger inputs and larger filters. Originally, the architecture was split into two columns/streams, but this was a workaround for the hardware limitations at the time. 

It used the ImageNet dataset which contained color images with nature objects of larger size compared to MNIST ($469\times 387$ vs $28 \times 28$) and 1.2 million images vs 60k images.

Its key improvements on the LeNet were

\begin{itemize}
    \item Add a dropout layer after two hidden dense layers (better robustness /regularization)
    
    Dropout allowed for much deeper networks by introducing regularization not just at the input layer but throughout multiple layers of the network. This made it possible to control the complexity of the model more effectively.

    \item Change activation function from sigmoid to ReLu (no more vanishing gradient)
    
    enabling training of deeper networks more efficiently.

    \item MaxPooling instead of Average Pooling
    
    This made the learned features more shift-invariant, which is important for object recognition. Max pooling generally retains the most salient features and discards less useful information, making the model more robust.

    \item Heavy data Augmentation like cropping, shifting, and rotation. 
    \item Model ensembling 
    
    Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model

    \item Using softmax function for classification
    \item Increase in kernel size and stride
    
    design choice to accommodate the higher resolution of images in the ImageNet dataset compared to MNIST.

\end{itemize}

AlexNet is substantially more complex, being about 250 times more computationally expensive. However, it is only ten times larger parametrically than LeNet-5. This way AlexNet is notorious for its high memory usage. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/AlexNetFigure.png}
    \caption{Architecture of AlexNet}
\end{figure}

\lstinputlisting[language=python]{code/AlexNet.py}

\subsection{VGG}

The Visual Geometry Group~\cite{VGG} uses the `bigger means better' philosophy. VGG introduced the notion of repeated blocks.

\begin{itemize}
    \item \textbf{NOT Add more dense layers} - computationally expensive
    \item \textbf{NOT Add more convolutional layers} - as the network grows, specifying each convolutional layer individually becomes tedious
    \item \textbf{Group Layers into blocks} - these blocks can easily be parameterized, creating a more organized, modular architecture
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/VGGFigure.png}
    \caption{Architecture of VGG}
\end{figure}

\lstinputlisting[language=python]{code/VGG.py}

\subsubsection{fewer wide convolutions or more narrow convolutions?}

Recent comprehensive analysis from papers has shown that using more layers of narrow convolutions outperforms using fewer wide ones. This has been a general trend in network design: having more layers of simpler functions is generally more powerful than fewer layers of more complex functions. 

\subsubsection{The VGG block}

Several $3\times 3$ convolutions, padded by one maintains the spatial dimensions from the input to the output layer, and at the end, max-pooling layer of $ 2\times2$ and stride of 2 halves the resolution. 

Combining these blocks with dense layers, creates an entire family of architectures just by varying the number of blocks.

\subsubsection{Performance}

VGG tends to be a lot slower when compared to the throughput of AlexNet (Figure~\ref{fig:architecture-comparison}), however, it makes up for in terms of accuracy. While VGG might require more computational resources, it generally provides superior performance.

\subsection{Inception}

\begin{figure}
    \centering
    \subfigure[Inception Architecture]
        {
            \begin{turn}{90}
                \includegraphics[width=23.2cm, height=0.6\textheight, keepaspectratio]{figures/Inception1-2-300x67-4041309011.png}
            \end{turn}
        }
    \subfigure[Simplified Inception architecture]
        {
            \begin{turn}{90}
                \includegraphics[width=\linewidth]{figures/inception-simplified-view.png}
            \end{turn}
        }
    \caption{Inception code can be found at \texttt{http://d2l.ai} chapter named 8.4. Multi-Branch Networks (GoogLeNet)}
\end{figure}

The Inception architecture~\cite{Inception} is both deep and introduces the concept of parallel paths within the network which offers multiple pathways for data to flow. The architecture combines the best of different types of convolutions and pooling layers to enhance its performance.

The decision was:

\begin{quote}
    if you opt for $ 5\times5$ convolutions, you will end up with many parameters, leading to a lot of computational cost and possibly overfitting, even though it might be more expressive. On the other hand, if you go with $ 1\times1$ convolutions, you'll have a more controlled, memory-efficient architecture, but it may not perform as well.
\end{quote}

``The most straightforward way of improving the performance of deep neural networks is by increasing their size. This includes both increasing the depth - the number of levels - of the network and its width: the number of units at each level. This is as an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data''~\cite{Inception}


This comes with two major drawbacks
\begin{itemize}
    \item Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. 
    
    This can become a major bottleneck, since the creation of high quality training sets can be tricky and expensive, especially if expert human raters are necessary to distinguish between fine-grained visual categories like those in ImageNet
    \item Another drawback of uniformly increased network size is the dramatically increased use of computational resources. For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then a lot of computation is wasted.
\end{itemize}

\begin{figure}[H]
    \centering
    \subfigure[Inception module, naive version]{%
        \includegraphics[width=.45\linewidth, height=0.3\textheight, keepaspectratio]{figures/inception-naiive.png}\label{fig:inception-naive}
    }
    \subfigure[Inception module with dimension reductions]{%
        \includegraphics[width=.45\linewidth, height=0.3\textheight, keepaspectratio]{figures/inception-detailed.png}\label{fig:inception-detailed}
    }
    \caption{Inception Module}
\end{figure}

\subsubsection{Naiive Inception Module}

``In order to avoid patch-alignment issues, current incarnations of the Inception architecture are restricted to filter sizes $ 1\times1$, $ 3\times3$ and $ 5\times5$, however this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success in current state of the art convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (Figure~\ref{fig:inception-naive})''~\cite{Inception}.

The Problem with the naiive approach is that ``even a modest number of $ 5\times5$ convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters. This problem becomes even more pronounced once pooling units are added to the mix: their number of output filters equals to the number of filters in the previous stage. The merging of the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage. Even while this architecture might cover the optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within a few stages''~\cite{Inception}.

\subsubsection{Detailed Inception Module}

This leads to a second proposed architecture (Figure~\ref{fig:inception-detailed}). `` judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to model. We would like to keep our representation sparse at most places and compress the signals only whenever they have to be aggregated en masse. That is, $ 1\times1$ convolutions are used to compute reductions before the expensive $ 3\times3$ and $ 5\times5$ convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose''~\cite{Inception}

\subsubsection{How the Modules fit together}

``As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease suggesting that the ratio of $ 3\times3$ and $ 5\times5$ convolutions should increase as we move to higher layers''~\cite{Inception}.

Now, you might be wondering, how do all these different types of convolutions fit together in the Inception block? The key to making this work is using appropriate padding. For example, the 3$\times$3 convolutions use half-padding by one, and the 5$\times$5 convolutions use half-padding by two. This ensures that the dimensions of the inputs and outputs align and remain the same. Once that's taken care of, you simply stack all these layers together.

\subsubsection{Channels}

The first inception block has channels sizes specified.

The first Inception block uses 64 channels for the 1$\times$1 convolutions. For the 3$\times$3 convolutions, it uses 128 channels. And for the 5$\times$5 convolutions, 32 channels are used, primarily because 5$\times$5 convolutions already come with a large number of parameters—25 times 32 in this case. When it comes to max pooling, a few other dimensions are included. The goal is to have all these different parts sum up to 256 channels. 

\subsubsection{Advantages}

The Inception block is designed to use a relatively low number of parameters (better than just striaght up sticking with one dimension of convolution) and floating-point operations (FLOPs), without sacrificing performance.

It has less computational operations:

\begin{equation*}
    k^2 \times \stackrel{fixed}{\boxed{c_{in}}} \times c_{out} \times \stackrel{fixed}{\boxed{m_h \times m_w}}
\end{equation*}

Consider a convolutional kernel of size $k\times k$, $c$ relating to the channels and dimension of the image $m$. 

We have the flexibility to adjust $c_{in} \wedge c_{out}$.

\begin{equation*}
    \stackrel{fixed}{\boxed{c_{in} \times m_h \times m_w}} \times \sum_{j \in paths}\bigl(k^2_j \times c_{out,j}\bigr)
\end{equation*}

By carefully allocating resources - varying the number of channels and kernel sizes - we can optimize the network's performance.

\subsubsection{Channels}

The Inception blocks are designed in such a way that they need fewer parameters and less computational complexity than a single 3x3 or 5x5 convolutional layer, as shown in Table 1. If we were to have 256 channels in the output layer, Inception needs only 16,000 parameters and costs only 128 Mega FLOPS, whereas a 3x3 convolutional layer will need 44,000 parameters and cost 346 Mega FLOPS, and a 5x5 convolutional layer will need 1,22,000 parameters and cost 963 Mega FLOPS. So Inception blocks, essentially get the same job done as single convolutional layers, with much better memory and compute efficiency. \href{https://medium.com/swlh/understanding-inception-simplifying-the-network-architecture-54cd31d38949}{from here}

\subsection{ReSNet}

\subsubsection{Motivation: Does adding layers improve accuracy?}

Gradients don't properly propagating through many layers. Once you add more layers, you change the function class, making it more versatile but also different; More layers means more parameters, granting us the power to model more complex functions.

As you add more layers, the function class becomes more powerful, but also diverge - adding more layers might get you closer to this truth initially, but after a certain point you could drift away.

In an ideal world, you'd like more layers to create a new function which neatly nests the previous function - making this similar to the last.  The goal is to have function classes that are both increasingly powerful and nested as we add layers.

\subsubsection{Residual Networks}

ResNet~\cite{ResNet} parameterizes around an identity function $f(x)=x$ instead of a zero function $f(x)=0$. Now, as you tune your parameters, you start to diverge from this identity function. The beauty is that you don't have to learn the identity function from scratch, reducing unnecessary computational overhead. It implies a different inductive bias and allows for the addition of new layers without disrupting the outputs of previous layers.

Now, the functions don't precisely implement nested function classes, but it comes remarkably close in an approach called the ResNet block (Figure~\ref{fig:resblock}), where the input is added to the output residual.

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{figures/ResBlock.png}
    \caption{Residual learning: building block from~\cite{ResNet}}
    \label{fig:resblock}
\end{figure}

Each block generally contains two or more convolutions, each followed by a batch normalization and a Rectified Linear Unit (ReLU). The innovation lies in adding the original input back to the output of this sequence. In some cases, we might run a 1x1 convolution on the input before adding it to the output, especially if we need to change the dimensionality of the input to match the output.

This setup makes it easier for the network to learn an identity mapping, essentially enabling the network to decide whether or not a change is beneficial.

\lstinputlisting{code/ResNet.py}

\subsubsection{The Architecture}

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{figures/resnet-block.png}
    \caption{ResNet Module, code at \texttt{http:d2l.ai} 8.6. Residual Networks (ResNet) and ResNeXt}
\end{figure}

A typical ResNet module may include a block with down-sampling; this usually employs a stride of 2. Following this, several other standard ResNet blocks are stacked. Down-sampling in each module is commonly performed using a stride of 2, serving to reduce the spatial dimensions of the feature maps. To introduce complexity and non-linearity into the network, 1x1 convolutions are often employed within the ResNet blocks. Finally, these blocks are stacked together to form a deep and complex network architecture.

Residual connections are a defining feature that contribute to the network's expressiveness and allow for training deeper models without facing vanishing or exploding gradients. To manage dimensionality, ResNet uses pooling and strides just like earlier architectures. Batch normalization layers are interspersed throughout to control the capacity and stabilize the training process.

\subsection{DenseNet}

Densely Connected Convolution Networks~\cite{DenseNet} have the characteristic that each layer in a DenseNet are connected to every other layer in a feed-forward manner, quite unlike traditional CNNs where each layer is only connected to its immediate successor.

\begin{itemize}
    \item The key advantage of this dense connectivity is feature reuse: each layer gets the feature maps from all the preceding layers, not just the last one. This makes it efficient and requires fewer parameters for high performance. So, you get a network that's both computationally and memory-efficient.

    \item Dense connections also help alleviate the vanishing gradient problem by improving the gradient flow through the network, making optimization easier.
    
    \item It is also scalable, since you can easily adjust the number of layers and other hyper-parameters to make it suitable for a variety of tasks and data sizes.
\end{itemize}

Occasionally need to reduce resolution (transition layer)

(Figures on presentation)

\subsection{SqueezeNet}

\begin{figure}[H]
    \centering
    \includegraphics[trim={0 0 0 0},clip,width=\linewidth]{figures/SqueezeNet.png}
    \caption{SqueezeNet architecture from~\cite{SqueezeNet}} \label{fig:squeezeNet-architecture}
\end{figure}

SqueezeNets~\cite{SqueezeNet} use attention mechanisms. 

\subsubsection{Attention}

In machine learning, attention allows a model to focus on significant features within the data, rather than indiscriminately treating all features equally. Think of it like spotlighting the essential parts in an otherwise noisy scene.

The paradox of needing to know what you are recognizing before you actually recognize it is addressed in the Squeeze-and-Excite Network (SE-Net) through the use of attention mechanisms.

\subsubsection{Implementation}

In a traditional network, information moves relatively slowly from layer to layer. This can mean that it takes several layers for information from one corner of an image to influence the other corner.

The network calculates a global descriptor for each channel. These descriptors go through a small neural network that outputs new descriptors. These new descriptors are then used to scale or 'excite' the original channels. This is computationally efficient and offers a more refined way to focus on what's crucial in the image.

\begin{enumerate}
    \item \textbf{Squeeze} -  takes a global average pooling of each channel to get a single
    descriptor that captures the global information of that channel.
    \item \item \textbf{Descriptor} - These descriptors are then fed into a small neural network (usually a fully-
    connected layer followed by a ReLU and another fully-connected layer) to create a new
    set of descriptors. These new descriptors are designed to capture which channels are
    more important given the current global context of the image
    \item \item \textbf{Excite} - The output descriptors are then used to scale the original channels. This scaling
    effectively "excites" or enhances the channels that are likely to be more important for a
    given task, based on the global context of the image
    \item \textbf{Re-weighting Channels} - So if the global context suggests that the image likely contains a cat, the "cat channel" would be scaled up, emphasizing features relevant to cats and making it easier for subsequent layers to detect the cat. This procedure is computationally efficient and allows the network to focus on relevant features without needing to explicitly know in advance what those features represent (like a "cat" or a "dog"). It's a way of allowing the network to dynamically allocate its attention based on the overall features in the image, resolving the paradox.
\end{enumerate}

\subsection{Evaluation}

\subsubsection{Performance}

\begin{figure}[H]
    \centering
    \includegraphics[trim={0 0 0 0},clip,width=.7\linewidth]{figures/bokeh_plot.png}
    \caption{The \href{https://cv.gluon.ai/model_zoo/classification.html}{performance} of the architectures} \label{fig:architecture-comparison}
\end{figure}

\subsubsection{What we learnt}

\begin{itemize}
    \item Dense layers are computationally and memory intensive. Real-world problems with big input tensors and many classes will prohibit their use.
    \item $1\times 1$ convolutions act like a multi-layer perceptron per pixel
    \item Scientists are humans and need a while to understand the power of new approaches. Eventually they do but a lot of vanity is involved in the process
    \item If not sure, just take all options and let the optimization decide or even learn this through trial and error (genetic algorithm, AmoebaNet)
    \item Deeper is not necessarily better if the function space is not regularized.
    \item ResNet is the workhorse of Deep Learning (For now) 
    \item Lot's of variations have been proposed but it often boils down to how you train a network and for what purpose
\end{itemize}

\section{Batch Norm}

The problem deep networks face is with convergence, making training difficult and time consuming. There is a strategy to perform ``deep supervision'' which involves backpropogating from intermediate stages to help the network learn - however, this has its limitations [TODO].

\subsection{Motivation}

Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously; gradients flow from the top layer down to the bottom layers of the network. As a result, the last layers start to adapt first, followed by the layers below them, creating a cascade of adaptations throughout the network. 

However, this leads to a problem: As the bottom layers adapt, they change the features that are fed back up to the top layers. This means that the top layers, which had already started to adapt, have to readjust to these new inputs. Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change~\cite{Goodfellow-et-al-2016}. \textbf{Each layer's learning destabilizes the next, causing a slow convergence process that takes a long time for all layers to adapt properly}.

\subsection{Solution}

Batch normalization can be applied to any input or hidden layer in a network~\cite{Goodfellow-et-al-2016}.

Batch Norm mitigates this issue by normalizing the features within each mini-batch, thereby stabilizing the training process and speeding up convergence. The idea is to make minor corrections to the layers by adjusting their mean and variance during training.

\begin{equation*}
    \mu_B = \frac 1 {|B|}\sum_{i\in B} x_i \quad \sigma^2_B = \frac 1 {|B|} \sum _{i\in B} (x_i - \mu_B)^2 + \epsilon
\end{equation*}

\begin{equation}
    x_{i+1} = \gamma \frac{x_i - \mu_B}{\sigma_B} + \beta\label{eq:batch-norm}
\end{equation}

\begin{center}
Where $\gamma$ is variance and $\beta$ is the mean. $|B|$ represents the size of the mini-batch, and $x_i$ are the individual data points in that mini-batch. $\epsilon$ is a small positive value such as $10^{-8}$ introduced to avoid the potential problem of undefined gradient.
\end{center}

We compute the mean and variance of a given mini-batch during training. Then, re-normalize each input feature by subtracting its mean and dividing it by its standard deviation. The model also learns two parameters to scale ($\gamma$) and shift ($\beta$) the normalized features. 

We then adjust these first and second moments separately from the rest of the network learning, leading to Equation~\ref{eq:batch-norm}. 

\subsection{Advantages}

This way, we are able to ``normalize'' each mini-batch separately, making the training of deep networks more stable and faster (dramatically reducing the number of training epochs required to train deep networks). The normalization can be undone by the network by learning appropriate $\gamma$ and $\beta$ parameters if it sees a benefit to it.

\subsection{Updated Aim}

The original motivation of batch normalization is reducing covariate shift. This is not correct. BatchNorm actually worsens covariate shift, yet it still aids in model convergence. 

\textbf{BatchNorm is effectively acting as a form of regularization by introducing noise into the model}. 

Here's how it works: You calculate the mean and variance of a mini-batch, let's say, of 64 observations. Since you're working with a small sample, both the mean and the variance are subject to noise. You then normalize the features using these noisy statistics, introducing a random scale and shift into your model at each batch. This random noise acts as a regularizing factor, which is why you often don't need additional regularization techniques like dropout when you're using BatchNorm. However, this property makes BatchNorm sensitive to the size of the mini-batch. 

If your mini-batch is too large, you're not introducing enough noise for effective regularization. If it's too small, the noise level becomes counterproductive, affecting convergence. This mini-batch size sensitivity becomes especially significant in multi-GPU settings, where batch sizes are often adjusted.

\subsection{Application}

If you're working with a dense layer, a single normalization is applied to all the activations in that layer.

In the case of a convolutional layer, a separate normalization is performed for each channel.

\subsection{Randomization during use}

You don't want this randomness when you're using the model for predictions. So, you fix the gamma and beta parameters that the model has learned during training. Instead of using batch statistics, you use the running average for the mean and variance to normalize the features.

\section{Data Augmentation}

\subsection{Input Augmentation}

Input data augmentation is a technique used to increase the size and diversity of your
training set. It does this by applying a series of random but realistic transformations to each data
point during training

By increasing the size of your training data artificially, you're effectively adding more "experiences" for your model, which can improve generalization.

It can be done through \href{https://github.com/aleju/imgaug}{imgaug} or \href{https://pytorch.org/vision/stable/transforms.html}{pytorch.transforms}.

One of the main benefits of data augmentation is its role as a regularizer. It helps prevent overfitting by ensuring that the model encounters a variety of different, yet plausible, examples during training.

\subsection{Transformations}

\begin{itemize}
    \item Random 
    \begin{itemize}
        \item flipping - can simulate the natural orientations of objects or scenes in images
        \item scaling - It helps the model generalize across different sizes of the same object or feature.
        \item rotations - add another layer of complexity by altering the angle of the data points. This is especially useful in tasks like object recognition, where orientation can vary widely.
        \item intensity/contrast variations - different lighting conditions and image qualities
        \item cropping/padding - alter the focus and frame of the data points. This can be helpful for tasks where the subject can be off-center or partially visible
        \item noise - serves as an effective way to improve the model's robustness. It mimics real-world scenarios where data may not always be clean or noise-free
        \item affine transformations - translation, scaling, and shearing. These offer another way to introduce variability into your data, helping your model generalize better
        \item perspective transformations -  alter the viewpoint of the object or scene. This helps in applications like augmented reality, where the perspective can dramatically change the appearance of object
    \end{itemize}
\end{itemize}

\subsection{Anomaly detection}

Pick out the unusualities out of one type of class. 

\subsubsection{Predicit Continuation}

You would train something that is able to regress, to predict the continuation of the function. Then you train some sort of auto encoder that is able from a limited set of inputs to continue this function. If you continue this function and the prediction is too different from your external observation, you reflect that it is an outlier.

\subsubsection{Measure distance in Latent Space}

TODO

\subsubsection{Reconstruct the input}

If you have an auto-encoder, you need to take an input image and reconstruct it. After training, in theory, the network would have a hard time reconstructing something that it has never seen - so it will likely remove it from the image, which means that if you then take the output reconstruction and subtract from the external input which has the anomaly then the error will get highlighted.

\subsubsection{Classify artificial, subtle variations - out of distribution detection}

\subsection{Approaches}

\subsubsection{Unsupervised}

Use auto-encoder reconstruction error and use moving averages, dropout and set time window

\subsubsection{Supervised}

RNNs Learn form a set of yes/nos ina  time series. RNNs can learn from a series of time steps and predict when the anomaly is about to occur.

\subsubsection{Streaming and minibatches}

\printbibliography

\end{document}