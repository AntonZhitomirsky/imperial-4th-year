{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4xov4cobBw3y"
   },
   "source": [
    "#Deep Learning T04 - Batch Normalization\n",
    "\n",
    "In this exercise notebook, you will implement the Batch Normalization (BN) operator in Pytorch, verify that your implementation passes a selection of tests, and answer/discuss a few questions.\n",
    "\n",
    "Implementing BN involves a number of steps:\n",
    "\n",
    "1. Inputting batch $x_i$, saved moving averages of mean and variance of activations\n",
    "2. Defining learnable parameters $\\gamma$ and $\\beta$\n",
    "3. Calculating mini-batch mean $\\mu_i$\n",
    "4. Calculating mini-batch variance $\\sigma^2_i$\n",
    "5. Normalizing $x_i$ to have zero mean and unit variance across batch dimension, $\\hat{x}_i$ = $\\frac{x_i - \\mu_i}{\\sigma^2_i  + \\epsilon}$\n",
    "6. Scaling and shifting using learnable parameters: $z_i = \\gamma \\odot \\hat{x}_i - \\beta$\n",
    "7. Update moving averages of mean and variance\n",
    "\n",
    "At test time, the mini-batch operations in steps 3 and 4 are replaced with moving averages of the mean and variance that are computed during training.\n",
    "\n",
    "Additionally, we will only implement BN for the following inputs:\n",
    "\n",
    "* Output of a fully connected layer (shape: (batch_size, L))\n",
    "\n",
    "    * Mean and var computed to retain **feature dimension, L**\n",
    "\n",
    "* Output of a convolutional layer (shape: (batch_size, C, H, W))\n",
    "\n",
    "    * Mean and var computed to retain **channel dim, C**\n",
    "\n",
    "Note: passing the tests is not a guarantee that your implementation is perfect.\n",
    "Check the model answer when it is released to confirm that your implementation is correct. **Try not to look at the test code unless you get completely stuck, as this may give hints about how to complete the exercise.**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GNWZ0LRLOZWA"
   },
   "source": [
    "**Questions**:\n",
    "1. Why do we use $\\epsilon$ in the denominator when normalizing the input?\n",
    "2. Why do we use moving averages of the mean and variance at test time to compute the BN operator?\n",
    "3. Is there any need for a bias term in the layer preceding the BN operator? Explain.\n",
    "4. What kind of modifications to the learning rate might BN enable?\n",
    "5. How might BN affect the network's sensitivity to weight initialization?\n",
    "6. How might the batch size relate to BN's suggested regularization effect?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NaFh5jDTO5q_"
   },
   "source": [
    "**Answers**:\n",
    "1. To prevent division by zero.\n",
    "2. We do not want noisy mean and var estimates at test time, and also sometimes these may not be available (e.g. single observation prediction).\n",
    "3. Batch norm already contains a bias term so this is unnecessary.\n",
    "4. BN may enable using larger learning rates.\n",
    "5. BN may make training less sensitive to magnitudes of weights at init.\n",
    "6. It has been suggested that noise in mean and variances estimates in batch norm arising from smaller batch sizes may contribute to the regularizing effect that batch norm appears to have."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fnWWy0S9IslR"
   },
   "source": [
    "Complete the following skeleton by **replacing any lines where variables are set to placeholder `torch.zeros(1)` or `torch.ones(1)` and adding any other code you need** and verify the tests in cell below pass. **Do not modify any other provided code or variable names** as this may break the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xCaFcPDoATds"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, shape, eps=1e-5, momentum=0.9):\n",
    "        \"\"\"\n",
    "        shape: Expected shape of input\n",
    "        eps: epsilon used in normalization step\n",
    "        momentum: momentum value used to update moving averages\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if len(shape) not in (2, 4):\n",
    "            raise ValueError(\"Invalid input shape!\")\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # PART 1: defining Pytorch learnable parameters (hint: use `shape` argument\n",
    "        # which should depend on whether the input is from FC or Conv). NOTE:\n",
    "        # tests expect that broadcasting will be used in PART 8 below, so define\n",
    "        # your parameters accordingly.\n",
    "        # UPDATE:\n",
    "        if len(shape) == 2:\n",
    "            _shape = (1, shape[1])\n",
    "        else:\n",
    "            _shape = (1, shape[1], 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(_shape))\n",
    "\n",
    "        # PART 2: initialize moving avg variables (hint: these are NOT learnable \n",
    "        # parameters)\n",
    "        self.moving_mu = torch.zeros_like(self.gamma)\n",
    "        self.moving_sigma = torch.ones_like(self.gamma)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Test\n",
    "        if not torch.is_grad_enabled():\n",
    "            # PART 3: Test time normalization operation; use self.eps as epsilon\n",
    "            # UPDATE:\n",
    "            x_hat = (x - self.moving_mu) / torch.sqrt(self.moving_sigma + self.eps)\n",
    "\n",
    "            # Logging code for tests; ignore:\n",
    "            self._tmp_x_hat_test = x_hat\n",
    "        \n",
    "        # Training\n",
    "        else:\n",
    "            if len(x.shape) == 2:\n",
    "                # PART 4: Compute mean and var for FC input (retaining feature dim)\n",
    "                # UPDATE:\n",
    "                mean = x.mean(dim=0)\n",
    "                var = x.var(dim=0)\n",
    "\n",
    "            elif len(x.shape) == 4:\n",
    "                # PART 5: Compute mean and var for Conv input (retaining channel dim)\n",
    "                # UPDATE (hint: use `keepdim` flag to use broadcasting later):\n",
    "                mean = x.mean(dim=(0, 2, 3), keepdim=True)\n",
    "                var = x.var(dim=(0, 2, 3), keepdim=True)\n",
    "            else:\n",
    "                raise ValueError(\"Incorrect input shape!\")\n",
    "            \n",
    "            # Logging code for tests; ignore:\n",
    "            self._tmp_mean = mean\n",
    "            self._tmp_var = var\n",
    "\n",
    "            # PART 6: Training time normalization operation; use self.eps as epsilon\n",
    "            # UPDATE:\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "            # Logging code for tests; ignore:\n",
    "            self._tmp_x_hat_train = x_hat\n",
    "            \n",
    "            # PART 7: Updating moving averages; use self.momentum to calculate\n",
    "            # contribution to update (hint: be careful about unnecessary \n",
    "            # autograd computation tracking)\n",
    "            # UPDATE:\n",
    "            self.moving_mu = self.momentum * self.moving_mu + (1.0 - self.momentum) * mean.data\n",
    "            self.moving_sigma = self.momentum * self.moving_sigma + (1.0 - self.momentum) * var.data\n",
    "\n",
    "            # Logging code for tests; ignore:\n",
    "            self._tmp_moving_mu = self.moving_mu\n",
    "            self._tmp_moving_sigma = self.moving_sigma\n",
    "\n",
    "        # PART 8: Scale and shift x_hat using learnable parameters to compute output\n",
    "        # UPDATE:\n",
    "        z = self.gamma * x_hat + self.beta\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQq5sMOvN9bH",
    "outputId": "bb717fa0-a7e9-4304-b052-eac48d1344c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10/10 TESTS PASSED:\n",
      "#################\n",
      "0 - Learnable parameters set correctly\t: Pass\n",
      "1 - Learnable parameter shapes correct\t: Pass\n",
      "2 - Moving average shapes correct\t: Pass\n",
      "3 - Test time normalization\t\t: Pass\n",
      "4 - Mean computation\t\t\t: Pass\n",
      "5 - Variance computation\t\t: Pass\n",
      "6 - Train time normalization\t\t: Pass\n",
      "7 - Train time mean moving average\t: Pass\n",
      "8 - Train time variance moving average\t: Pass\n",
      "9 - Final scale and shift\t\t: Pass\n"
     ]
    }
   ],
   "source": [
    "#@title To run the tests, first run your code in the cell above to define your BN implementation, and then run this block. Do not read code for this block until solutions released!\n",
    "torch.manual_seed(0)\n",
    "\n",
    "test_conv_input = torch.randn(64, 16, 32, 32)\n",
    "test_fc_input = torch.randn(64, 128)\n",
    "\n",
    "def test_is_param():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    if not bn_fc.gamma.requires_grad:\n",
    "        return False\n",
    "    if not bn_fc.beta.requires_grad:\n",
    "        return False\n",
    "    if not bn_conv.gamma.requires_grad:\n",
    "        return False\n",
    "    if not bn_conv.beta.requires_grad:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def test_param_shapes():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    if bn_fc.gamma.shape != (1, 128):\n",
    "        return False\n",
    "    if bn_fc.beta.shape != (1, 128):\n",
    "        return False\n",
    "    if bn_conv.gamma.shape != (1, 16, 1, 1):\n",
    "        return False\n",
    "    if bn_conv.beta.shape != (1, 16, 1, 1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def test_ma_shapes():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    if bn_fc.moving_mu.shape != (1, 128):\n",
    "        return False\n",
    "    if bn_fc.moving_sigma.shape != (1, 128):\n",
    "        return False\n",
    "    if bn_conv.moving_mu.shape != (1, 16, 1, 1):\n",
    "        return False\n",
    "    if bn_conv.moving_sigma.shape != (1, 16, 1, 1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def test_test_norm():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    bn_fc.moving_mu = torch.randn((1, 128))\n",
    "    bn_fc.moving_sigma = torch.rand((1, 128)) + 0.01\n",
    "    bn_conv.moving_mu = torch.randn((1, 16, 1, 1))\n",
    "    bn_conv.moving_sigma = torch.rand((1, 16, 1, 1)) + 0.01\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            bn_fc(test_fc_input)\n",
    "            bn_conv(test_conv_input)\n",
    "\n",
    "            tmp_fc_xhat = bn_fc._tmp_x_hat_test\n",
    "            expected = (test_fc_input - bn_fc.moving_mu) / torch.sqrt(bn_fc.moving_sigma + bn_fc.eps)\n",
    "            if not torch.allclose(expected, tmp_fc_xhat):\n",
    "                return False\n",
    "\n",
    "            tmp_conv_xhat = bn_conv._tmp_x_hat_test\n",
    "            expected = (test_conv_input - bn_conv.moving_mu) / torch.sqrt(bn_conv.moving_sigma + bn_conv.eps)\n",
    "            if not torch.allclose(expected, tmp_conv_xhat):\n",
    "                return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_train_mean():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    try:\n",
    "        bn_fc(test_fc_input)\n",
    "        bn_conv(test_conv_input)\n",
    "\n",
    "        bn_fc_mean = bn_fc._tmp_mean\n",
    "        expected_fc = test_fc_input.mean(dim=0)\n",
    "                \n",
    "        bn_conv_mean = bn_conv._tmp_mean\n",
    "        expected_conv = test_conv_input.mean(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "        if not torch.allclose(bn_fc_mean, expected_fc):\n",
    "            return False\n",
    "        \n",
    "        if not torch.allclose(bn_conv_mean, expected_conv):\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Test 4 - trace: \", e)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test_train_var():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "    try:\n",
    "        bn_fc(test_fc_input)\n",
    "        bn_conv(test_conv_input)\n",
    "\n",
    "        bn_fc_var = bn_fc._tmp_var\n",
    "        expected_fc = test_fc_input.var(dim=0)\n",
    "                \n",
    "        bn_conv_var = bn_conv._tmp_var\n",
    "        expected_conv = test_conv_input.var(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "        if not torch.allclose(bn_fc_var, expected_fc):\n",
    "            return False\n",
    "        \n",
    "        if not torch.allclose(bn_conv_var, expected_conv):\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"Test 5 - trace: \", e)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_train_norm():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "\n",
    "    try:\n",
    "        bn_fc(test_fc_input)\n",
    "        bn_conv(test_conv_input)\n",
    "        \n",
    "        expected_fc_mean = test_fc_input.mean(dim=0)\n",
    "        expected_fc_var = test_fc_input.var(dim=0)\n",
    "        bn_fc_xhat = bn_fc._tmp_x_hat_train\n",
    "        expected_fc = (test_fc_input - expected_fc_mean) / torch.sqrt(expected_fc_var + bn_fc.eps)\n",
    "        \n",
    "        expected_conv_mean = test_conv_input.mean(dim=(0, 2, 3), keepdim=True)\n",
    "        expected_conv_var = test_conv_input.var(dim=(0, 2, 3), keepdim=True)                \n",
    "        bn_conv_xhat = bn_conv._tmp_x_hat_train\n",
    "        expected_conv = (test_conv_input - expected_conv_mean) / torch.sqrt(expected_conv_var + bn_conv.eps)\n",
    "\n",
    "        if not torch.allclose(bn_fc_xhat, expected_fc):\n",
    "            return False\n",
    "        if not torch.allclose(bn_conv_xhat, expected_conv):\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"Test 6 - trace: \", e)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test_train_mov_mean():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "\n",
    "    cached_fc_mean = torch.randn((1, 128))\n",
    "    cached_conv_mean = torch.randn((1, 16, 1, 1))   \n",
    "\n",
    "    bn_fc.moving_mu = cached_fc_mean\n",
    "    bn_conv.moving_mu = cached_conv_mean\n",
    "\n",
    "    expected_fc_mean = test_fc_input.mean(dim=0).data\n",
    "    expected_conv_mean = test_conv_input.mean(dim=(0, 2, 3), keepdim=True).data\n",
    "\n",
    "    try:\n",
    "        bn_fc(test_fc_input)\n",
    "        bn_conv(test_conv_input)\n",
    "\n",
    "        bn_fc_moving_mu = bn_fc._tmp_moving_mu\n",
    "        bn_conv_moving_mu = bn_conv._tmp_moving_mu\n",
    "\n",
    "        expected_fc = bn_fc.momentum * cached_fc_mean + (1.0 - bn_fc.momentum) * expected_fc_mean\n",
    "        expected_conv = bn_conv.momentum * cached_conv_mean + (1.0 - bn_conv.momentum) * expected_conv_mean\n",
    "\n",
    "        if not torch.allclose(expected_fc, bn_fc_moving_mu):\n",
    "            return False\n",
    "        if not torch.allclose(expected_conv, bn_conv_moving_mu):\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Test 7 - trace: \", e)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_train_mov_var():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "\n",
    "    cached_fc_sigma = torch.rand((1, 128)) + 0.01\n",
    "    cached_conv_sigma = torch.rand((1, 16, 1, 1)) + 0.01\n",
    "    \n",
    "    bn_fc.moving_sigma = cached_fc_sigma\n",
    "    bn_conv.moving_sigma = cached_conv_sigma\n",
    "\n",
    "    expected_fc_var = test_fc_input.var(dim=0)\n",
    "    expected_conv_var = test_conv_input.var(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "    try:\n",
    "        bn_fc(test_fc_input)\n",
    "        bn_conv(test_conv_input)\n",
    "\n",
    "        bn_fc_moving_sigma = bn_fc._tmp_moving_sigma\n",
    "        bn_conv_moving_sigma = bn_conv._tmp_moving_sigma\n",
    "\n",
    "        expected_fc = bn_fc.momentum * cached_fc_sigma + (1.0 - bn_fc.momentum) * expected_fc_var\n",
    "        expected_conv = bn_conv.momentum * cached_conv_sigma + (1.0 - bn_conv.momentum) * expected_conv_var\n",
    "\n",
    "        if not torch.allclose(expected_fc, bn_fc_moving_sigma):\n",
    "            return False\n",
    "        if not torch.allclose(expected_conv, bn_conv_moving_sigma):\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Test 8 - trace: \", e)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_scale_and_shift():\n",
    "    bn_fc = BatchNorm(test_fc_input.shape)\n",
    "    bn_conv = BatchNorm(test_conv_input.shape)\n",
    "\n",
    "    try:\n",
    "        res_fc = bn_fc(test_fc_input)\n",
    "        res_conv = bn_conv(test_conv_input)\n",
    "\n",
    "        expected_fc = bn_fc.gamma * bn_fc._tmp_x_hat_train + bn_fc.beta\n",
    "        expected_conv = bn_conv.gamma * bn_conv._tmp_x_hat_train + bn_conv.beta\n",
    "\n",
    "        if not torch.allclose(expected_fc, res_fc):\n",
    "            return False\n",
    "        if not torch.allclose(expected_conv, res_conv):\n",
    "            return False\n",
    "\n",
    "        if res_fc.shape != test_fc_input.shape:\n",
    "            return False\n",
    "        if res_conv.shape != test_conv_input.shape:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Test 9 - trace: \", e)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def run_tests():\n",
    "    results = {\n",
    "        \"0 - Learnable parameters set correctly\\t\" : test_is_param(),\n",
    "        \"1 - Learnable parameter shapes correct\\t\" : test_param_shapes(),\n",
    "        \"2 - Moving average shapes correct\\t\" : test_ma_shapes(),\n",
    "        \"3 - Test time normalization\\t\\t\" : test_test_norm(),\n",
    "        \"4 - Mean computation\\t\\t\\t\" : test_train_mean(),\n",
    "        \"5 - Variance computation\\t\\t\" : test_train_var(),\n",
    "        \"6 - Train time normalization\\t\\t\" : test_train_norm(),\n",
    "        \"7 - Train time mean moving average\\t\" : test_train_mov_mean(),\n",
    "        \"8 - Train time variance moving average\\t\" : test_train_mov_var(),\n",
    "        \"9 - Final scale and shift\\t\\t\" : test_scale_and_shift()        \n",
    "    }\n",
    "    total = sum([v for v in results.values()])\n",
    "    print()\n",
    "    print(\"{}/10 TESTS PASSED:\".format(total))\n",
    "    print(\"#################\")\n",
    "    for k, v in sorted(results.items()):\n",
    "        print(\"{}: {}\".format(k, \"Pass\" if v else \"*FAIL*\"))\n",
    "\n",
    "run_tests()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bn_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
