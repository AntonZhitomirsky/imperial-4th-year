{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P1wXZDG-wupJ"
      },
      "source": [
        "# Tutorial 6 - Attention and Transformers\n",
        "This tutorial consists of three parts.\n",
        "1. In [Part 1](#part1) you will try and get a better understanding for how and why softmax attention works\n",
        "2. In [Part 2](#part2) you will derive and investigate the positional encoding schemes used in transformer models when encoding sequences\n",
        "3. In [Part 3](#part3) you will implement the attention and position encoding components of a State-of-the-Art Object-Centric Learning architecture\n",
        "4. In [Part 4](#part4) you will derive a lower-complexity form of Softmax attention introduced in the recent Performer paper\n",
        "\n",
        "**Notes**\n",
        "* Only Part 1 (Multi-headed Attention) and Part 2 (Position Encodings) are directly related the course material (i.e. examinable). They are also very important in modern DL! - see the [\"10 Novel Applications using Transformers\"](https://paperswithcode.com/newsletter/3#10_novel_applications_using_transformers) section \n",
        "* A list of relevant and [further reading](#refs) is provided at the end of the document. In some cases this tutorial is based on these materials\n",
        "* You can navigate this notebook easily through the _table of contents_ on the left sidebar\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hraWPjPHgIRS"
      },
      "outputs": [],
      "source": [
        "# Setup Colab - might take 30 sec\n",
        "!pip install -q tensorflow-cpu tensorflow_datasets \\\n",
        "     jax git+https://github.com/deepmind/dm-haiku flax optax "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WlRknqG18hCz"
      },
      "source": [
        "<a name=\"part1\"></a>\n",
        "# Part 1: Attention \n",
        "Here we will try and understand how attention came about in NLP by viewing Attention as soft-dictionary querying. If you get stuck see [Attention References 1](#refs) on which this question is based.\n",
        "\n",
        "We'll only need numpy here (see [this cheatsheet](https://github.com/harrygcoppock/ImperialMScAIUtils/blob/main/cheatsheets/numpy-cheatsheet.pdf) for a refresher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsGv43tDgFb5"
      },
      "outputs": [],
      "source": [
        "# Imports \n",
        "import numpy as np \n",
        "from flax.nn import dot_product_attention # For testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6BHqsrZadnrs"
      },
      "source": [
        "## Implementing a Dictionary with matrices\n",
        "We are all familiar with dictionaries which store `key: value` pairs, permitting the lookup of a `value` through a `query` which must match (exactly) its corresponding `key`. We will see that attention is essentially a form of soft dictionary lookup, where our queries are vectors, and our output will be a matrix containing _all_ the value vectors in our dictionary, weighted by the degree to which our query matched the corresponding key. \n",
        "\n",
        "\n",
        "We'll start by defining a matrix of values and corresponding keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXGvlyyOHJA9"
      },
      "outputs": [],
      "source": [
        "values = np.array([[1,9],\n",
        "                   [2,8],\n",
        "                   [4,7],\n",
        "                   [8,6]])\n",
        "\n",
        "keys   = np.array([[1,0,0],\n",
        "                   [0,1,0],\n",
        "                   [1,0,0],\n",
        "                   [0,0,1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnRMO3DJg-j7"
      },
      "outputs": [],
      "source": [
        "# Now Let's perform a query using the dot product\n",
        "query = np.array([0,1,0]) # This matches the second key\n",
        "query_match = np.dot(query, keys.T)\n",
        "query_match"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HvzC6hnzhYXT"
      },
      "source": [
        "We can see that when a query matches a key perfectly, the dot product of the query vector and keys matric returns a one-hot vector. \n",
        "\n",
        "**Question** Think about which operation you can perform using the `query_match` vector, to retrieve the corresponding value from the `values` matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjIeSxlShy22"
      },
      "outputs": [],
      "source": [
        "matched_value = ... #SOLUTION np.dot(query_match, values)\n",
        "print(\"Well Done\" if np.array_equal(matched_value, np.array([2,8])) else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmmjGp5idKl"
      },
      "source": [
        "Think about the reason this worked so nicely. In particular:\n",
        "1. Why did the `query_match` take the form of a one-hot vector, allowing us to extract a value perfectly? \n",
        "<font color=\"green\">There are two reasons. Firstly, the keys and queries were both unit vectors (row-wise). Secondly, there was only one key which matched the query exactly</font>\n",
        "2. What happens when this isn't the case? (There are two separate behaviours worth investigating)\n",
        "<font color=\"green\">The good thing: we can get a weighted mix of all the values when we don't have exact matches (good for NNs). The bad thing: if the query.value vector isn't normalized, we'll end up with a scenario where }}$\\Sigma_i{\\lambda_i} \\neq 1 \\: \\color{green}{\\text{(here}}\\: \\mathbb{\\lambda} \\:\\color{green}{\\text{is the vector query_match)}}$</font>\n",
        "\n",
        "Once you have thought about these questions, we will try to address an issue with the more general case - we'll also call the `query_match` vector `scores` from now on, as it essentially gives us a smooth similarity score between queries and keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJAOqI-Ykbem"
      },
      "outputs": [],
      "source": [
        "# In the more general case our query or keys may not be normalized\n",
        "query = np.array([0,3,0])\n",
        "scores = np.dot(query, keys.T)\n",
        "scores"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5bAxES2cmIqj"
      },
      "source": [
        "You can see that if we extract one (or more generally, multiple) values which match this query, they will be multiplied by weights which do not sum to 1. This is not desirable, so let's fix it (I highly recommend using [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) - [see tutorial](https://rockt.github.io/2018/04/30/einsum), though it's not needed here):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ywdyfktmVde"
      },
      "outputs": [],
      "source": [
        "# Modify scores so that it is always normalized \n",
        "scores = scores / np.sum(scores, axis=-1) # For batched case, we'll specify the axis\n",
        "print(\"Well Done\" if np.array_equal(scores, np.array([0,1,0,0])) else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DF9eaIdQsIUg"
      },
      "source": [
        "It would be better still if the query match_vector wasn't completely soft, but actually disproportinately weighted up those components of the vector which gave a significant match... I wonder which function we've met before that might do this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zru1FX-OsEWg"
      },
      "outputs": [],
      "source": [
        "# Which function might we use to get a normlized, positive and sharply directed vector?\n",
        "query = np.array([200,3,-1])\n",
        "scores = np.dot(query, keys.T) \n",
        "scores = ... #SOLUTION  np.exp(scores) / np.sum(np.exp(scores), axis=-1) # Softmax!\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR5uYJtKmxyi"
      },
      "outputs": [],
      "source": [
        "# Great! Now let's put this into a function which takes a batch of queries, keys and values\n",
        "# and returns a batch of softly-matched values\n",
        "def soft_lookup(queries, keys, values):\n",
        "\n",
        "    # SOLUTION\n",
        "    # We'll use einsum to avoid fiddling with shapes due to batching\n",
        "    scores = np.einsum('bqd,bkd->bqk', queries, keys)\n",
        "    scores = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
        "    matched_values =  np.einsum('bqk,bkv->bqv', scores, values)\n",
        "    return matched_values\n",
        "    # SOLUTION\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE0PYAzpnAXE"
      },
      "outputs": [],
      "source": [
        "# We'll test this against a built-in soft attention implementation from flax\n",
        "# This requires reshaping to specify single-headed attention\n",
        "# Batch size 1, 3 key:value pairs, keys have 2 floats, values have 6 floats\n",
        "queries = np.random.rand(1,4,1,2) # 4 queries\n",
        "keys = np.random.rand(1,3,1,2)\n",
        "values = np.random.rand(1,3,1,6)\n",
        "\n",
        "ours = soft_lookup(queries[:,:,0,:]/np.sqrt(queries.shape[-1]), keys[:,:,0,:], values[:,:,0,:])\n",
        "theirs = np.squeeze(dot_product_attention(queries, keys, values, deterministic=True), axis=-2)\n",
        "\n",
        "print(\"Well Done\" if np.isclose(ours, theirs, atol=1e-4).all() else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_KdAtxqA1e0a"
      },
      "source": [
        "Is you look carefully, you'll notice that we pass in the query divided by the dimension of the keys/queries. This is part of scaled dot-product attention,\n",
        "\n",
        "$Attention (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{n}}\\right) \\mathbf{V},$\n",
        "\n",
        "and is \"motivated by the concern when the input is large, the softmax function may have an extremely small gradient\" which would result in slow learning (from [Lillian Weng's blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html))."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4rV5mslGdhTr"
      },
      "source": [
        "## From the Attention Operation to Self-Attention Layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCsE-1y-Y_-"
      },
      "source": [
        "<figure>\n",
        "   <img src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/felix_hill_selfattention.png?raw=true\" alt=\"Self-attentin operating on word-embeddings\" width=\"100%\">\n",
        "   <figcaption align=\"center\">Figure 1 - Self-attention in Transformers. \n",
        "Taken from <a src=\"https://www.youtube.com/watch?v=8zAP2qWAsKg&feature=youtu.be&ab_channel=DeepMind\">Felix Hill's NLP Lecture</a> - <a src=\"https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L7%20-%20UCLxDeepMind%20DL2020.pdf\">slides</a>.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VxN-9QPX3rtf"
      },
      "source": [
        "As you saw in lecture, we can use this soft-attention operation on top of learned embeddings to perform *fast* sequence processing (in comparison to RNNs). Let's quickly recap the key-advantages of self-attention over RNNs:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwk2tVG20IuA"
      },
      "source": [
        "1. Applying self-attention over a sequence is faster than using an RNN on modern hardware, why?\n",
        "<font color=\"green\">\n",
        "RNNs have to iterate through every word in the sequence - this is not parallelizable, which makes it slower than huge matrix multiplications on GPU/TPU hardware </font>\n",
        "\n",
        "2. Capturing relationships between elements of a sequence which are far apart is easier with self-attention than RNNs, why? \n",
        "<font color=\"green\"> Two related reasons - 1. RNNs have limited capacity in their \"memory\", and there is always some loss of information between subsequent time steps. Neither of these apply to the key-query operation which is applied across the entire sequence. 2. It is easier for self-attention mechanisms to learn long-range dependencies as the gradient path is much shorter (especially through stacked attention layers, as in transformers) . Even if your RNN doesn't have \"vanishing gradients\", long-range effects will still be washed out. </font> \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K6BEmyJFxi-v"
      },
      "source": [
        "The key idea of self-attention is to generate a query, key and value vector for every word in the _entire_ sequence by applying the same weights (often just a fully-connected 1 layer MLP, i.e. a Linear transformation). As such we are learning a projection from the word-embedding space to some sub-space which will hopefully capture meaningful aspects of words, and relationships between them."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_L-xIihayHjR"
      },
      "source": [
        "It's clear then why this is called _self_-attention, as the output of the layer for every word will be a new embedding consisting of the score-weighted values of all embeddings in the previous layer. As such, every word in the sequence attends to every other word (and itself!), so the sequence is \"self-attending\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2OVVfsHl4Ykd"
      },
      "source": [
        "## Multi-Head Self-Attention\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "55q1Fkcz2KqC"
      },
      "source": [
        "\n",
        "If you try to intuitively grasp what might be occuring in these sub-spaces, it may seem strange that we assume one set of transformations will be rich enough to capture all kinds of interesting relationships between embeddings (especially as there is no guarantee that our embeddings are particularly semantically rich themselves). For example, if the word embedding of \"beetle\" and \"bee\" are very similar, then so too will be their queries and keys; this may be fine if we are handling generic data, such that the downstream parts of our model will handle these well as a result. However, if we are training out model on the entonomological literature, we might need to learn transformations that are very sensitive to the small differences in the \"beetle\" and \"bee\" embeddings, whilst still handling other words well - given that our transformations are just 1-layer MLPs though, this may not occur.\n",
        "\n",
        "<figure>\n",
        "   <img src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/felix_hill_multihead.png?raw=true\" alt=\"Multi-head Attention operating on word-embeddings\" width=\"100%\">\n",
        "   <figcaption align=\"center\">Figure 2 - Multi-Head Self-Attention in Transformers. \n",
        "Modified from <a src=\"https://www.youtube.com/watch?v=8zAP2qWAsKg&feature=youtu.be&ab_channel=DeepMind\">Felix Hill's NLP Lecture</a> - <a src=\"https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L7%20-%20UCLxDeepMind%20DL2020.pdf\">slides</a>.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "As such, we use \"multi-headed\" attention, where we learn a set of linear transformation, each applied (still in parallel) across the sequence. Then our model can learn to map to distinct sub-spaces, which ideally end up capturing different aspects of the items in our sequence.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzdH-0Jv-b2V"
      },
      "source": [
        "One nuance in transformers is that the linear transformations applied by each attention head are typically chosen such that the resulting concatenation of each head's outputs will equal the size of the input embedding (so $W_q$ is rectangular with shape `[dim_embedding,  dim_embedding/num_head]`).\n",
        "\n",
        "Beyond a few tricks such as using layer normalization and skip connections, that's pretty much all there is to transformers! Now let's have a go at implementing multi-headed attention:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F0M1Yn7_yvy"
      },
      "outputs": [],
      "source": [
        "\"\"\" For this example we will suppose that we have:\n",
        "    - A sequence of three words, each with an embedding dimension of 4 floats\n",
        "    - Two attention heads, whose output shape will be chosen such that the \n",
        "      output of the layer will still be [3, 4] in shape\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "# First we'll create our artifical sequence\n",
        "sequence = np.random.rand(3,4) # We'll ignore batching as it just adds a dim\n",
        "\n",
        "# And some linear transformations (without bias for simplicity)\n",
        "# For every head -> a W matrix for each of K, Q, V  - Overall matrix will be 4D\n",
        "head_weights = np.random.rand(...) # You need to specify the shape! #SOLUTION (2,3,4,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRD7rDGoA_zM"
      },
      "outputs": [],
      "source": [
        "# Now let's carry out the multiheaded attention operation\n",
        "kqv_matrix = ... # SOLUTION np.einsum('hpwd,tw->hptd', head_weights, sequence)\n",
        "print(\"Well Done\" if kqv_matrix.shape == (2,3,3,2) else \"Try again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VheykC4sDELl"
      },
      "outputs": [],
      "source": [
        "# We'll break up the keys, queries and values so we can apply our function\n",
        "k, q, v = np.rollaxis(kqv_matrix, axis=SPECIFY) # Be careful with the axis # SOLUTION axis 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK3US-bKDAha"
      },
      "outputs": [],
      "source": [
        "# Now we can use the batched function from earlier, \n",
        "# using the batch dimension to treat the multiple heads\n",
        "ours = soft_lookup(q/np.sqrt(q.shape[-1]), k, v)\n",
        "\n",
        "# Now specify a function to concenate along the head dimension\n",
        "concat = lambda x: x # SOLUTION Many ways - np.concatenate(np.rollaxis(x, axis=0), axis=-1)\n",
        "ours = concat(ours)\n",
        "\n",
        "print(\"Well Done\" if ours.shape == (3,4) else \"Try again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb7PgDr_EDFS"
      },
      "outputs": [],
      "source": [
        "# Again, we'll compare to flax's built-in dot_product_attention\n",
        "from flax.nn import dot_product_attention\n",
        "transform = lambda x: np.expand_dims(np.array(x).transpose(1,0,2), 0)\n",
        "theirs = concat(transform(dot_product_attention(*map(transform, (q, k, v)),\n",
        "                                       axis=1, deterministic=True)[0])[0])\n",
        "print(\"Well Done\" if np.isclose(ours, theirs, atol=1e-4).all() else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6AmTdjugH0"
      },
      "source": [
        "Great Stuff! You'll be seeing a **fun** use-case for these ideas at the end of Part 2, but first, let's take a step back and talk about Position Encodings."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mjrhjrpqHJv_"
      },
      "source": [
        "<a name=\"part2\"></a>\n",
        "# Part 2: Position Encodings"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3eBAH_dzyjqg"
      },
      "source": [
        "## Properties of Position Encodings\n",
        "As you may have noticed in Part 1, none of the operations underlying the `soft-lookup` function were dependant on the ordering of the data. As transformers are typically used to process sequences (or more recently images), we need another way to ensure that information about position is being utilized by the network. \n",
        "\n",
        "This is where position encodings come in - by adding or concatenating information about the position to every element in the model's input sequence we allow the network to leverage positional information. However, we can't use just any position encoding if we want to give the network a reasonable chance of learning such relationships.\n",
        "\n",
        "Ideally, we'd like the following to hold for our choice of positional encoding:\n",
        "\n",
        "1. The encodings should be determinstic and unique for every word in the sentence\n",
        "2. Distance (within sequence) based differences in value should not depend on sequence length\n",
        "\n",
        "\n",
        "The above two properties allow the network to reason about _absolute_ positions and _relative_ position within sequences in a consistent fashion. Finally we also want the encodings to:\n",
        "\n",
        "3. Be well-behaved for sequences of unseen length (e.g. defined over any domain, whilst maintaining above properties)\n",
        "\n",
        "The discussion in the remainder of this part will follow Amirhossein Kazemnejad's [blog post](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/), so if you get stuck you might look there."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OajYJVWCyozn"
      },
      "source": [
        "## Transformers - Sinusoidal Position Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zHCesk8muV6H"
      },
      "source": [
        "In the original [Transformer paper](https://arxiv.org/abs/1706.03762) Vaswani et al. introduce a specific position encoding which uses the sine and cosine functions to (practically) satisfy all of the above properties (they also tried using a learnable position embeddings and found that these gave similar performance, but were less likely to generalize to sequences of unseen length). Their scheme provides a $d$-dimensional encoding, $p^{(t)}$ for every element $t$ in the input sequence, where the elements of this encoding are given by:\n",
        "$$\n",
        "p^{(t)}_{i\\leq d} =\\left\\{\\begin{array}{ll}\n",
        "\\sin \\left(\\omega_{k}t\\right), & \\text { if } i=2 k \\\\\n",
        "\\cos \\left(\\omega_{k} t\\right), & \\text { if } i=2 k+1\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "with \n",
        "$$\n",
        "\\omega_k = \\frac{1}{10000^{2k/d}}=10^{-\\frac{8k}{d}}\n",
        "$$\n",
        "\n",
        "Let's try and understand why and how this encoding works - we'll begin by seeing whether it satisfies the properties which we require from a good position encoding (except determinism which is trivially satisfied).\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMu0tA6u4-A"
      },
      "source": [
        "<h3> Uniqueness and Absolute Positions </h3>\n",
        "\n",
        "Before considering relative distance, let's check that this encoding is unique and preserves some notion of absolute position in the sequence. We'll do this by implementing the function and taking a look at the results.\n",
        "\n",
        "Below there are two functions `position_encoding` and `get_angles` which you should complete. Remember that the `position_encoding` produces a $d$ dimensional vector for each $d$ dimensional word embedding (token) in the input sequence, so that these can be added before feeding into the Transformer's attention layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GNTnzrP8CXv"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "tokens = 10 # i.e Sequence Length\n",
        "token_dim = 64\n",
        "\n",
        "# SOLUTION\n",
        "# Code based on https://www.tensorflow.org/tutorials/text/transformer\n",
        "# SOLUTION\n",
        "\n",
        "def get_angles(pos, i, d):\n",
        "    \"\"\"\n",
        "        in:\n",
        "            pos - token positions : shape [L, 1]\n",
        "            i   - pos encoding indices : shape [1, d]\n",
        "            d   - pos encoding size \n",
        "        out:\n",
        "            encoding_angles - angles in radians : shape [L, d]\n",
        "\n",
        "    \"\"\"\n",
        "    # Returns a vector of angles given by omega (vector) * token position\n",
        "    \n",
        "    # SOLUTION\n",
        "    angle_rates = np.power(10, (-8 * (i//2)) / np.float32(d))\n",
        "    encoding_angles = pos * angle_rates\n",
        "    # SOLUTION\n",
        "\n",
        "    return encoding_angles\n",
        "\n",
        "def position_encoding(position, d):\n",
        "    \"\"\"\n",
        "        in: \n",
        "            position - token position (integer)\n",
        "            d - token dimension size (d)\n",
        "        out: \n",
        "            positions encodings - array shape [L, d]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # angle_rads = get_angles()\n",
        "    pos_enc = None\n",
        "\n",
        "    # SOLUTION\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d)[np.newaxis, :],\n",
        "                            d)\n",
        "    \n",
        "    \n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_enc = angle_rads\n",
        "    # SOLUTION\n",
        "\n",
        "    return pos_enc\n",
        "\n",
        "p = position_encoding(tokens, token_dim)\n",
        "print(\"Well Done\" if p.shape==(tokens,token_dim) else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-CRtOoQCZriA"
      },
      "source": [
        "Now we'll visualize these encodings and try to reason about whether, and how, they satisfy the properties we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0FJ3jlC7cal"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "tokens = 48\n",
        "token_dim = 86\n",
        "p = position_encoding(tokens, token_dim) \n",
        "\n",
        "f, ax = plt.subplots(2, 2, figsize=(12,10))\n",
        "\n",
        "# Plot 1 - A few components\n",
        "ax[0,0].plot(np.arange(token_dim), p[[4,5,tokens-2,tokens-1], :].T)\n",
        "ax[0,0].set_title(\"Plot 1 - Some Encoding Dimensions\")\n",
        "ax[0,0].legend([\"Seq Posn %d\"%l for l in [4,5,tokens-2,tokens-1]])\n",
        "ax[0,0].set_xlabel(\"Encoding Dimension\")\n",
        "ax[0,0].set_ylabel(\"Encoding Magnitude\")\n",
        "\n",
        "# Plot 2 - The position encoding matrix\n",
        "pcm = ax[0,1].pcolormesh(p, cmap='coolwarm')\n",
        "ax[0,1].set_title(\"Plot 2 - Encoding Matrix\")\n",
        "ax[0,1].set_xlabel('Encoding Dimensions')\n",
        "ax[0,1].set_ylabel('Token Position')\n",
        "f.colorbar(pcm, ax=ax[0,1])\n",
        "\n",
        "# Plot 3 - Two components and their neighbors\n",
        "colors = ['magenta', 'cyan']\n",
        "style = [':', \"-\", \"--\"]\n",
        "for i, enc_dim in enumerate([8,token_dim-24]):\n",
        "    for j, nghbor in enumerate([-4,0,4]):\n",
        "        ax[1,0].plot(np.arange(tokens), (p[:, enc_dim+nghbor]-p[:, enc_dim]),\n",
        "                   label=f\"Dim {enc_dim}+{nghbor}\", c=colors[i], linestyle=style[j])\n",
        "ax[1,0].set_title(\"Plot 3 - Neighboring Dimension Differences\")\n",
        "ax[1,0].set_xlabel(\"Token Position\")\n",
        "ax[1,0].set_ylabel(r\"$Neighbors - Base$ (arbitrary units)\")\n",
        "ax[1,0].legend()\n",
        "ax[1,0].set_yticks([])\n",
        "\n",
        "# Plot 4 - Dot Product of encodings across time\n",
        "pcm2 = ax[1,1].pcolormesh(p.dot(p.T), cmap='coolwarm')\n",
        "ax[1,1].set_title(\"Plot 4 - Dot-product Distances Across Sequence\")\n",
        "ax[1,1].set_xlabel('Token Position')\n",
        "ax[1,1].set_ylabel('Token Position')\n",
        "f.colorbar(pcm2, ax=ax[1,1])\n",
        "\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "57Bp-EhB7zHC"
      },
      "source": [
        "Now let's try and analyze these plots. Address the following questions:\n",
        "1. What does Plot 1 tell use about the correspondence between the encoding dimensions and a) Phase Differences b) Magnitude Differences\n",
        "2. What do Plots 1 and 2 tell us about the overall magnitude of the encodings. Is this desirable?\n",
        "3. What do Plots 1 and 2 tell us about the relevance of the first vs. last encoding dimensions?\n",
        "4. What do Plots 3 and 4 tell us about the way in which relative distances are encoded?\n",
        "\n",
        "Based on your considerations of the above, do you think that this position encoding satisfied _all_ of the criteria we laid out earlier?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kGsCn-7UjtQO"
      },
      "source": [
        "**Your Answer**\n",
        "\n",
        "1. <font color=\"green\">For a given token in our sequence, there is no phase difference between the embedding dimensions, but there is a magnitude difference.</font>\n",
        "\n",
        "2. <font color=\"green\">Given their sinusoidal form, the encodings are bounded between -1 and 1. This is desirable as it means we do not need to perform additional scaling to prevent the addition of the encodings to the word embeddings from dominating numerically.</font>\n",
        "\n",
        "3. <font color=\"green\">The initial dimensions in the encoding are far more sensitive to token position than the latter ones. As such, combinations thereof will more strongly inform the attention mechanism of absolute and relative token position. As we are adding encodings to word embeddings, we might expect that the network will learn to imbue the final dimensions with semantically relevant information, to better preserve the position information in the leading dimensions.</font>\n",
        "\n",
        "4. <font color=\"green\">Trivially, we again see that the final dimensions contain very little position information.Conversely, it is clear that neighboring differences in the leading dimensions are very significant, and distinct, such that these can be leveraged by the network fairly easily. Plot 4 also shows that the (dot product) distance between the encoding vectors is symmetric and smoothly (though not monotically) decaying over the sequence. As the encodings are _added_ to the embeddings, which are used to compute the $K$,$Q$ matrices, over which we take the dot-product, this suggests a strong distance-based contribution in the attention mechanism due to these encodings. \n",
        "</font>\n",
        "\n",
        "<font color=\"green\">\n",
        "Yes in practice. However, as the encoding is sinusoidal, it is not technically unique for all possible values of t, or k. However, the frequency is set very high (goes as $1/10000$), such that the \"wavelength\" with respect to the sequence length, $t$, is far longer than any relative-position distance we might care about anyway.\n",
        "</font>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O5XjRwz2TNyE"
      },
      "source": [
        "<h3>Relative Distances</h3>\n",
        "\n",
        "In order to publish our findings, and to keep you on your toes, we will now introduce some extraneous mathematical rigour. \n",
        "\n",
        "To do so, we'll find a linear transformation matrix $M$ which shifts a given sine-cosine pair, and is not a function of the absolute position, $t$, in the sequence. I.e. find some\n",
        "\n",
        "$$ M = \\left[\\begin{array}{ll}\n",
        "u_{1} & v_{1} \\\\\n",
        "u_{2} & v_{2}\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "such that \n",
        "\n",
        "$$\n",
        "M \\cdot\\left[\\begin{array}{l}\n",
        "\\sin \\left(\\omega_{k} t\\right) \\\\\n",
        "\\cos \\left(\\omega_{k} t\\right)\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "\\sin \\left(\\omega_{k} (t+\\phi)\\right) \\\\\n",
        "\\cos \\left(\\omega_{k} (t+\\phi)\\right)\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "_Hint_: You'll want to make use of the [trigonometric addition formula.](https://mathworld.wolfram.com/TrigonometricAdditionFormulas.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhnLcukmR523"
      },
      "source": [
        "**Your Answer**\n",
        "\n",
        "<font color=\"green\">\n",
        "SOLUTION\n",
        "<font/>\n",
        "\n",
        "<font color=\"green\">\n",
        "We want\n",
        "$$\n",
        "\\left[\\begin{array}{ll}\n",
        "u_{1} & v_{1} \\\\\n",
        "u_{2} & v_{2}\n",
        "\\end{array}\\right] \\cdot \\left[\\begin{array}{l}\n",
        "\\sin \\omega_{k} t) \\\\\n",
        "\\cos \\omega_{k} t)\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "\\sin (\\omega_{k} (t+\\phi)) \\\\\n",
        "\\cos (\\omega_{k} (t+\\phi))\n",
        "\\end{array}\\right]\n",
        "$$<font/>\n",
        "\n",
        "<font color=\"green\">\n",
        "Using the addition formula the RHS becomes\n",
        "$$\n",
        "=\\left[\\begin{array}{l}\n",
        "\\sin \\left(\\omega_{k}t\\right) \\cos \\left(\\omega_{k}  \\phi\\right)+\\cos \\left(\\omega_{k} t\\right) \\sin \\left(\\omega_{k}  \\phi\\right) \\\\\n",
        "\\cos \\left(\\omega_{k}  t\\right) \\cos \\left(\\omega_{k}  \\phi\\right)-\\sin \\left(\\omega_{k}  t\\right) \\sin \\left(\\omega_{k}\\phi\\right)\n",
        "\\end{array}\\right]\n",
        "$$<font/>\n",
        "\n",
        "<font color=\"green\">\n",
        "Multiplying this out gives the system of equations:\n",
        "$$\n",
        "\\begin{align}\n",
        "u_{1} \\sin \\left(\\omega_{k} t\\right)+v_{1} \\cos \\left(\\omega_{k} t\\right)&= \\:\\:\\: \\cos \\left(\\omega_{k} \\phi\\right) \\sin \\left(\\omega_{k}t\\right)+\\sin \\left(\\omega_{k} \\phi\\right) \\cos \\left(\\omega_{k} t\\right) \\\\\n",
        "u_{2} \\sin \\left(\\omega_{k}  t\\right)+v_{2} \\cos \\left(\\omega_{k} t\\right)&=-\\sin \\left(\\omega_{k} \\phi\\right) \\sin \\left(\\omega_{k} t\\right)+\\cos \\left(\\omega_{k} \\phi\\right) \\cos \\left(\\omega_{k}t\\right)\n",
        "\\end{align}\n",
        "$$<font/>\n",
        "\n",
        "<font color=\"green\">\n",
        "Here we can just match terms to get \n",
        "$$\n",
        "\\begin{array}{l}\n",
        "u_{1}=\\:\\:\\: \\cos \\left(\\omega_{k} \\phi\\right) \\quad v_{1}=\\sin \\left(\\omega_{k}  \\phi\\right) \\\\\n",
        "u_{2}=-\\sin \\left(\\omega_{k}  \\phi\\right) \\quad v_{2}=\\cos \\left(\\omega_{k} \\phi\\right)\n",
        "\\end{array}\n",
        "$$<font/>\n",
        "\n",
        "<font color=\"green\">\n",
        "And so we arrive at:\n",
        "$$\n",
        "M_{\\phi, k}=\\left[\\begin{array}{cc}\n",
        "\\: \\cos \\left(\\omega_{k} \\phi\\right) & \\sin \\left(\\omega_{k}  \\phi\\right) \\\\\n",
        "-\\sin \\left(\\omega_{k}  \\phi\\right) & \\cos \\left(\\omega_{k}  \\phi\\right)\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "<font/>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E4gZiCg66-vx"
      },
      "source": [
        "Explain how the existence of this M suggests that the self-attention layer should be able to learn about relative positions between tokens from encoding values alone."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pKXgt6mR7HmW"
      },
      "source": [
        "**Your Answer**\n",
        "\n",
        "<font color=\"green\">\n",
        "This M applies for any given sine-cosine pair for a given shift, so we can learn to relative positions as linear functions on top of the absolute position information coded by a given sine-cosine pair.\n",
        "</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FApYL8VnVr8X"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uerAL470HMeB"
      },
      "source": [
        "<a name=\"part3\"></a>\n",
        "# Part 3: Attention in Practice \n",
        "Now that we've covered the basics of attention and position encodings, we'll put these to use by completing the back-bone of a recent Object-Centric Learning architecture: Slot Attention Modules. \n",
        "\n",
        "You can feel free to skip this part (even though it's the most fun, in my unbiased opinion!), but you can also complete the questions with minimal understanding of how these modules work."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0OjzRORju74r"
      },
      "source": [
        "## Slot Attention Modules\n",
        "Slot Attention modules were introduced by Locatello et al. in [this paper](https://arxiv.org/abs/2006.15055), and Thomas Kipf gave an excellent [ICML Talk](https://slideslive.com/38930703/attentive-grouping-and-gnns-for-objectcentric-learning?ref=speaker-22634-latest) for those of you who wish to understand these a bit better (minutes 6:30-15:00 in particular). We'll be using these here as they are easy to understand and State-of-the-Art for Object-Centric Learning (different from segmentation as we are primarily interested in learning useful latent representations), and because I've been playing around with them recently!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DVMM4cEQvIuU"
      },
      "source": [
        "\n",
        "<table><tr>\n",
        "<td>\n",
        "  <p align=\"center\" style=\"padding: 10px\">\n",
        "    <img alt=\"Forwarding\" src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/thomas_kipf_slotattention.png?raw=true\" height=\"220\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">Figure 3.a - Slots compete over the feature map <br>via iterative Weighted Softmax Attention. <br>  The feature map is a stack of encoded-pixel vectors, <br> with a position encoding added.</em>\n",
        "  </p>\n",
        "</td>\n",
        "<td>\n",
        "  <p align=\"center\">\n",
        "    <img alt=\"Routing\" src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/thomas_kipf_slotatt_autoencoder.png?raw=true\" height=\"220\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">3.b - An Auto-Encoding architecture is used in conjunction with the module. <br> This allows the end-to-end training of the architecture through an (MSE) <br> reconstruction loss. Each slot is decoded seperately into RBGA images,<br> and the Alpha masks are used to weight the (sum) combination of these .</em>\n",
        "  </p>\n",
        "</td>\n",
        "</tr></table>\n",
        "\n",
        "<!-- <div>\n",
        "    <div style=\"float:left\">\n",
        "        <figure>\n",
        "            <img src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/thomas_kipf_slotattention.png?raw=true\" alt=\"Image of word embeddings with attention on top\" height=\"250\">\n",
        "            <figcaption align=\"left\">asfasd</figcaption>\n",
        "        </figure>\n",
        "    </div>\n",
        "    <div style=\"float:left\">\n",
        "        <figure>\n",
        "            <img src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/thomas_kipf_slotatt_autoencoder.png?raw=true\" alt=\"Image of word embeddings with attention on top\" height=\"250\">\n",
        "            <figcaption align=\"right\">sda</figcaption>\n",
        "        </figure>\n",
        "    </div>\n",
        "</div> -->\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wtplfLJfIKww"
      },
      "source": [
        "There are two key components of Slot Attention modules:\n",
        "1. The architecture uses \"slots\" to encode objects. Each slot can represent any object as they are initialized using the same learned distribution (a side-effect of this is that there is no persistent allocation of a given object to a given slot - something addressed by [AlignNet](https://www.deepmind.com/research/publications/AlignNet-Unsupervised-Entity-Alignment). A slot is just a vector which is used to generate queries over the image's features\n",
        "2. These slots compete (via attention) for different (ideally contiguous) regions of the feature-space, thus hopefully learning to each capture distinct objects\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o1shSvSBN8Sb"
      },
      "source": [
        "In the next two sections you'll implement your own position encoding, as well as the weighted soft-max attention (very similar to single-headed attention), and then using these the model will be trained for you.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VSo59G7jx89k"
      },
      "source": [
        "## Creating your own Position Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-POAB1udy3_Z"
      },
      "source": [
        "In order to easily allocate spatial information to the features (needed as we'll be using ordering-agnostic attention), and to allow us to visualize the attention scores,  the encoder is constructed such that the encoded features have the same width/heigth as the input images.\n",
        "\n",
        "As such, you'll need to create a position encoding with dimensions [width, height, X] where X represents the encoding dimension you choose. This encoding is then passed through a simple MLP to increase the encoding dimension to match the feature space, so that they can be added (X->dim_features). A naïve choice of position encoding would be the (x,y) coordinates of every point in the image (ideally normalized), such that X=2. This is not far from what the authors use (see figure 4).\n",
        "<table><tr>\n",
        "<td>\n",
        "  <p align=\"center\" style=\"padding: 10px\">\n",
        "    <img alt=\"Visualization of the Position Encodings used the Slot Attention paper.\" src=\"https://github.com/afspies/icl_dl_tut8/blob/master/figs/slotattn_posenc.png?raw=true\" height=\"320\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">Figure 4 - Visualization of the 4D position encoding used in the Slot Attention paper.</em>\n",
        "  </p>\n",
        "</td>\n",
        "</tr></table>\n",
        "\n",
        "Their encoding assigns 4 floats to every x,y point - although 2 dimensions would suffice (x,y) to uniquely label each point, their position encoding makes it easy to learn a good transformation from the position encoding to the encoder's feature space. For which two reasons do you think that might be?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5-SqoiQDbyF1"
      },
      "source": [
        "1. <font color=\"green\">Each 4 dim. position encoding is normalized, so the linear transformation is applied on the same scale for any position </font>\n",
        "2. <font color=\"green\">Low-magnitude values may not lead to good learning of a transformation by the MLP doing the up-scaling. As such, the fact that every (x,y) direction has a range going 0->1 and 1->0 means there should always be some dimension whose transformation is being learned with a significant gradient.</font>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UbDjet-ubuc6"
      },
      "source": [
        "Try and come up with a sensible position encoding and complete the function below. Keep in mind that at the very least:\n",
        "* Every coordinate point's encoding should be unique\n",
        "* The result will be added to the embedded feature map (after passing through an MLP), not concatenated, so consider the magnitude of your embeddings carefully\n",
        "\n",
        "\n",
        "You shouldn't spend too long on this. In fact, a simple normalized x,y encoding will still work fairly well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WsOjHpgDc_0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def build_grid(resolution: tuple) -> np.ndarray:\n",
        "    \"\"\"\n",
        "        Given the resolution of a 2D grid, this function returns a 3D numpy array\n",
        "        consisting of position encodings for every x,y point\n",
        "        in:\n",
        "            resolution  (W, H)\n",
        "        out:\n",
        "            position encodings - shape [W, H, dim_encoding] \n",
        "    \"\"\"\n",
        "    # SOLUTION - from paper\n",
        "    ranges = [np.linspace(0., 1., num=res) for res in resolution]\n",
        "    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
        "    grid = np.stack(grid, axis=-1)\n",
        "    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
        "    return np.concatenate([grid, 1.0 - grid], axis=-1)\n",
        "    # SOLUTION\n",
        "\n",
        "    # If you can't think of anything sensible, no encoding (np.zeros)\n",
        "    # will work somewhat, and normalized (x,y) is decent\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azyMnyCo14aP"
      },
      "outputs": [],
      "source": [
        "# Let's check the output has the right shape\n",
        "encoding = build_grid((10, 8))\n",
        "if encoding.ndim == 3 and encoding.shape[:2] == (10,8):\n",
        "    print(\"Well Done!\")\n",
        "else:\n",
        "    print(\"Try again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qhn4768vIFM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualize your position embedding\n",
        "plt.imshow(encoding[:,:, CHOOSE_DIM or SUM]) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y-YE4oCxdok2"
      },
      "source": [
        "Great! We're half way there"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi1jZMWgyHXh"
      },
      "source": [
        "## Implementing Weighted Dot-Product Attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KvZZx3NKN_hh"
      },
      "source": [
        "Slightly deviating from the familiar \"soft-lookup\" style single-head attention from Part 1, Locatello et al. choose to combine values with a weighted _mean_, rather than a weighted _sum_ over attention scores, helping to stabilize the iterative attention mechanism.\n",
        "\n",
        "In particular, the attention (elementwise) is the same as what we implemented in Part 1:\n",
        "\n",
        "$$\\operatorname{attn}_{i, j}=\\frac{e^{M_{i, j}}}{\\sum_{l} e^{M_{i, l}}} \\quad \\text{where} \\quad M=\\frac{k(inputs) \\cdot q(slots)^{T} }{\\sqrt{D_{slots}}} \\in \\mathbb{R}^{N \\times K}.$$\n",
        "\n",
        "But, rather than taking the dot product between the scores and values, we first scale the scores by the mean:\n",
        "$$\n",
        "\\text { updates }=W^{T} \\cdot v(\\text { inputs }) \\in \\mathbb{R}^{K \\times D} \\quad \\text { where } \\quad W_{i, j}=\\frac{\\operatorname{attn}_{i, j}}{\\sum_{l=1}^{N} \\operatorname{attn}_{l, j}}.\n",
        "$$\n",
        "\n",
        "The two tricky parts here are choosing the axes over which to apply softmax and the sum  - look at the indices in the denominators carefully.\n",
        "\n",
        "You'll now implement this in the function below. Please make sure that you assign the softmaxxed attention scores to the `attn` variable, so that the attention masks get visualized properly during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrzVpiECLlit"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp \n",
        "from jax.nn import softmax # Recommended, as they compute softmax on x-x_max  =>\n",
        "#                            more stable, but need to be careful with gradients\n",
        "\n",
        "# Please use jnp instead of np, as this will use GPU when the function is executed\n",
        "# All functions you need from np are in jnp too\n",
        "def weighted_dotproduct_attention(keys, slot_queries, values, get_attn=False):\n",
        "    \"\"\" Applies Weighted Dot-Product attention, with shapes\n",
        "\n",
        "        in:\n",
        "            slot_queries  [batch, num_slots, hidden_size=slot_size]\n",
        "            keys          [batch, encoding_size (image WxH), hidden_size=slot_size]\n",
        "            values        [batch, encoding_size (image WxH), hidden_size=slot_size]\n",
        "            get_attn FLAG -> whether to return attn scores as well as slot updates\n",
        "        out:\n",
        "            updates         [batch, num_slots, slot_size]\n",
        "    \"\"\"\n",
        "    attn_eps = 1E-8 # Add this before calculating weighted mean\n",
        "    slot_size = 32  # Remember to scale the query in the softmax\n",
        "    \n",
        "    attn = None    # Used to visualize which features slots are attending to\n",
        "    updates = None # Used to update slots via residual MLP\n",
        "    # SOLUTION\n",
        "    attn_logits = jnp.einsum('bnd,bkd->bnk', keys, slot_queries/jnp.sqrt(slot_size)) # Apply softmax attention and normalize over slots\n",
        "    attn = softmax(attn_logits, axis=-1)\n",
        "\n",
        "    weights = attn + attn_eps\n",
        "    weights = weights / jnp.sum(weights, axis=-2, keepdims=True)\n",
        "    updates = jnp.einsum('bnk,bnd->bkd', weights, values)\n",
        "    # SOLUTION\n",
        "   \n",
        "    return (updates, attn) if get_attn else updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f33Rc_HqOLmV"
      },
      "outputs": [],
      "source": [
        "# We'll load some data to test whether your function is working properly\n",
        "!wget -q -O /content/test_attention.npz https://github.com/afspies/icl_dl_tut8/blob/master/test_data/test_weighted_dotprod_attn.npz?raw=true\n",
        "q, k, v, test_out = np.load('/content/test_attention.npz')['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-eNo9bNO6w6"
      },
      "outputs": [],
      "source": [
        "out = weighted_dotproduct_attention(k, q, v)\n",
        "print(\"Well Done!\" if np.isclose(out, test_out, atol=1e-6).all() else \"Try again\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MV63fCtQPHOh"
      },
      "source": [
        "If the test is running without errors but failing (so your shapes match), double check that you are scaling the queries by the sqrt of the slot dimension size, and that you are applying softmax over the slot-dimension of the attention scores.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QDty3pEPLBfA"
      },
      "source": [
        "## Training Our Model\n",
        "\n",
        "Great! Now we'll train the model and take a look at the output - you don't need to do anything here except wait and keep your fingers crossed :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP3MtGLDx3nP"
      },
      "outputs": [],
      "source": [
        "# Download Data and get useful code\n",
        "!git clone -q https://github.com/afspies/icl_dl_tut8.git\n",
        "!wget -q https://storage.googleapis.com/multi-object-datasets/tetrominoes/tetrominoes_train.tfrecords\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM1E0j9Z796p"
      },
      "outputs": [],
      "source": [
        "from icl_dl_tut8.src.slotattention_training import train_model, load_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rQaHpyoE3oxo"
      },
      "source": [
        "We'll be using the Tetominoes dataset from https://github.com/deepmind/multi_object_datasets to train and test our model. Here we'll load and visualize some data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLuoxCKJSJj_"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "ds = load_data(\"./tetrominoes_train.tfrecords\", batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EncpIZkA-Ng_"
      },
      "outputs": [],
      "source": [
        "# Look at a sample of images\n",
        "import matplotlib.pyplot as plt\n",
        "image_batch = next(ds)\n",
        "f, ax = plt.subplots(2,2,figsize=(6,6))\n",
        "count = 0\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax[i,j].imshow(image_batch[count])\n",
        "        ax[i,j].set_xticks([])\n",
        "        ax[i,j].set_yticks([])\n",
        "        count += 1\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HnudXmaZxnFs"
      },
      "source": [
        "Now we'll train the model - this will take around 15 minutes, so you might want to get a cup of coffee and some popcorn: Watching your loss curves is the true DL Researcher experience. You should start to see sensible results after about 20k steps, and good looking ones around 40-50k. \n",
        "\n",
        "You'll want to click the settings button (cog) in tensorboard and enable auto-reload, so you don't have to keep refreshing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjBLANpLvKhq"
      },
      "outputs": [],
      "source": [
        "!rm -rf logs # Remove old logs\n",
        "%tensorboard --logdir logs\n",
        "train_model(ds, weighted_dotproduct_attention, build_grid)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ATiZwiKyBZaA"
      },
      "source": [
        "Note that we chose to use four slots here as all images in the dataset contain exactly three tetrominoes. By looking at the attention masks throughout training, comment on:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrZbQQHg_Cp"
      },
      "source": [
        "1. What do the different slots attend to in the image? Are all of the slots needed here? <font color='green'>Ideally each slot wil attend to one distinct object, such that three slots will be attending to a tetromino each, and one slot will just be attending to the background. As the background is blank here, three slots would suffice.</font>\n",
        "2. Given that the position encoding provides a fairly weak contribution to the feature embedding (it was simply up-transformed and added after all), can you imagine situations in which the competition between slots is not strong enough? (you may even have observed this)\n",
        "<font color='green'> When objects are close in the image and have a similar colour, a single slot may end up attending to multiple objects. It is also possible for some objects to not be attended to at all, though this is not as common in this dataset, where all objects are large and have distinct colours </font>\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mf-oBkXSJdP9"
      },
      "source": [
        "<a name=\"part4\"></a>\n",
        "# Part 4: Making Transformers Faster\n",
        "In lecture we've seen that the attention operation grows quadratically with the sequence length. Here we'll derive a faster version, and get you to work out it's complexity - the fast attention operation, FAVOR+, was introduced in the recent [Performer](https://arxiv.org/abs/2009.14794) paper from Google.\n",
        "\n",
        "The run-time analysis is the most important question in this part, and you do not need to answer the more mathematical questions to be able to carry it out.\n",
        "\n",
        "This question _very closely_ follows Teddy Koker's excellent [blog post](https://teddykoker.com/2020/11/performers/), so if you get stuck that may be helpful."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WA7w3DhgyQ49"
      },
      "source": [
        "## The Kernel Trick and The Squared Exponential Kernel\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DYyb-uHeGFMn"
      },
      "source": [
        "Recall that we can use the kernel trick to solve problems of a certain form in a transformed space without needing to perform the computaionally expensive procedure of first mpapping every datapoint to the new space; instead, we can find a kernel expressed purely in terms of dotproducts over the data, which can be used to solve the transformed version of the problem.\n",
        "\n",
        "For this to be useful here we'll first need to express the Softmax operation in terms of a kernel (this is easy as we're taking the Softmax of queries dotted with keys).\n",
        "\n",
        "Starting with our dot-product softmax attention over queries, $Q\\in\\mathbb{R}^{N\\times d}$, keys, $K\\in\\mathbb{R}^{M\\times d}$, and values, $V\\in\\mathbb{R}^{M\\times l}$:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V,$$\n",
        "where we'll ignore the $d^{\\frac{-1}{2}}$ from here on out, and assume that $d^{\\frac{-1}{4}}$ was absorbed into both the $Q$ and $K$. Now note that the row-wise Softmax operation here corresponds to\n",
        "\n",
        "$$\\text{Softmax}(QK^T)_i = \\frac{e^{Q_iK_i^T}}{\\sum_i e^{Q_i K_i^T}} = \\frac{A_i}{\\sum_i A_i}, $$\n",
        "where we've defined \n",
        "\n",
        "$$ A := \\exp(QK^T) \\in \\mathbb{R}^{N\\times M}.$$ \n",
        "\n",
        "In order to write the entire operation in terms of matrices we'll define the row-vector of ones $L_d = [1,...,1] \\in\\mathbb{R}^{1\\times d}$, and then note that \n",
        "\n",
        "$$ \\begin{align} \n",
        "    A L_d &=\n",
        "        \\begin{bmatrix}\n",
        "           \\sum_j A_{0j} \\\\\n",
        "           \\vdots \\\\\n",
        "           \\sum_j A_{Nj}\n",
        "        \\end{bmatrix}.\n",
        "\\end{align}\n",
        "$$\n",
        "Defining $D=\\text{diag}(AL_d)$ we get \n",
        "\n",
        "\n",
        "$$ \\begin{align}\n",
        "D^{-1} &= \\begin{bmatrix}\n",
        "           \\frac{1}{\\sum_j A_{0j}} &  \\dots & 0 \\\\\n",
        "           \\vdots & \\ddots & \\vdots \\\\\n",
        "           0 &\\dots & \\frac{1}{\\sum_j A_{Nj}} \\\\       \n",
        "        \\end{bmatrix}.\n",
        "\\end{align}      \n",
        "$$\n",
        "Using $D^{-1}$ we can then write the row-wise softmax as \n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=D^{-1}AV.$$\n",
        "\n",
        "I would suggest covincing yourself that this holds using a generic $2\\times2$ $K$ and $Q$. Armed with this reformulation, we'll try to find a way of approximating $A$ which is better than $\\mathcal{O}(NMd)$ (where usually we have $N=M=\\text{seq_length}$. To do this, we define the Softmax kernel between two vectors\n",
        "$$K_{softmax}(x_i,x_j)=\\exp(x_i^T x_j).$$\n",
        "\n",
        "To get yourself warmed up, rewrite $A_{ij}$ in terms of this Softmax kernel, by considering the row-wise decomposition of $Q = [q_0,\\dots, q_N]^T$ and $K = [k_0,\\dots, k_M]^T$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8qw__N0V5sZ_"
      },
      "source": [
        "**Your Answer**\n",
        "\n",
        "$$A_{ij} = \\dots K_{softmax}(...)^{...} \\dots$$\n",
        "\n",
        "<font color='green'> $$A_{ij} = K_{softmax}(q_i^T, \\: k_j^T)$$ </font>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7CILfxVi7OQ7"
      },
      "source": [
        "We will see shortly that some clever people came up with a way of approximating the gaussian (or Radial Basis Function) kernel. To leverage this, we'll need to rewrite our softmax kernel in terms of guassian kernels:\n",
        "\n",
        "$$K_{guass}(x_i, x_j) = \\exp(-\\gamma \\|x_i-x_j\\|^2),$$\n",
        "where $\\gamma$ is some constant whose value you will choose."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cjk_Kg708Ibs"
      },
      "source": [
        "**Your Answer**\n",
        "Express the softmax kernel in terms of exponential terms and the gaussian kernel.This will take a few steps of calculation\n",
        "\n",
        "\n",
        "$$K_{softmax}(x_i,x_j) = \\dots K_{gauss}(x_i, x_j)^{...} \\dots $$\n",
        "\n",
        "<font color='green'>\n",
        "SOLN\n",
        "$$\n",
        "\\begin{aligned}\n",
        "K_{\\text {gauss }}\\left(x_{i}, x_{j}\\right) &=\\exp \\left(-\\gamma\\left\\|x_{i}-x_{j}\\right\\|^{2}\\right) \\\\\n",
        "&=\\exp \\left(-\\gamma\\left(\\left\\|x_{i}\\right\\|^{2}+\\left\\|x_{j}\\right\\|^{2}-2\\left(x_{i}^{\\top} x_{j}\\right)\\right)\\right) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "Now set $\\gamma=\\frac{1}{2}$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "K_{\\text {gauss }}\\left(x_{i}, x_{j}\\right) &=\\exp \\left(-\\frac{\\left\\|x_{i}\\right\\|^{2}}{2}-\\frac{\\left\\|x_{j}\\right\\|^{2}}{2}+x_{i}^{\\top} x_{j}\\right) \\\\\n",
        "&=\\exp \\left(\\frac{\\left\\|x_{i}\\right\\|^{2}}{2}\\right)^{-1} \\exp \\left(x_{i}^{\\top} x_{j}\\right) \\exp \\left(\\frac{\\left\\|x_{j}\\right\\|^{2}}{2}\\right)^{-1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Bringing the exponentials over to the other side gives\n",
        "$$\n",
        "\\exp \\left(\\frac{\\left\\|x_{i}\\right\\|^{2}}{2}\\right) K_{\\text {gauss }}\\left(x_{i}, x_{j}\\right) \\exp \\left(\\frac{\\left\\|x_{j}\\right\\|^{2}}{2}\\right)=\\exp \\left(x_{i}^{\\top} x_{j}\\right)=K_{\\text{Softmax}}\\left(x_{i}, x_{j}\\right)\n",
        "$$\n",
        "</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Le44KwycCs"
      },
      "source": [
        "## Approximation via Sampling\n",
        "In an [acclaimed 2007 paper](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf),  Rahimi and Recht showed that it is possible to approximate a kernel corresponding to the inner product in some $d$ dimensional space with an approximate feature map to a lower dimensional space $D << d$:\n",
        "\n",
        "$$K(x_i, x_j)=\\phi(x_i)^T\\phi(x_j) \\approx z(x_i)^T z(x_j), $$\n",
        "\n",
        "In particular, they showed that the guassian kernel can be approximated with the feature mapping\n",
        "\n",
        "$$\n",
        "z_{\\omega}^{gauss}(x)=\\left[\\begin{array}{l}\n",
        "\\cos \\left(\\omega^{\\top} x\\right) \\\\\n",
        "\\sin \\left(\\omega^{\\top} x\\right)\n",
        "\\end{array}\\right],\n",
        "$$\n",
        "\n",
        "where $\\omega \\sim \\mathcal{N}_D (0, I)$. \n",
        "\n",
        "Using this result, and your expression for the softmax kernel in terms of the guassian kernel above, write out which form $z^{softmax}_\\omega(x_i)$ will take for our approximate softmax kernel."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpEQNKGFH83"
      },
      "source": [
        "**Your Answer**\n",
        "\n",
        "<font color=\"green\">\n",
        "Have that \n",
        "$$\n",
        "\\exp \\left(\\frac{\\left\\|x_{i}\\right\\|^{2}}{2}\\right) K_{\\text {gauss }}\\left(x_{i}, x_{j}\\right) \\exp \\left(\\frac{\\left\\|x_{j}\\right\\|^{2}}{2}\\right)=\\exp \\left(x_{i}^{\\top} x_{j}\\right)=K_{\\text{softmax}}\\left(x_{i}, x_{j}\\right)\n",
        "$$\n",
        "\n",
        "and want $z(x_i)$ such that $z(x_i)^T z(x_j) =K_{\\text{softmax}}\\left(x_{i}, x_{j}\\right)$. Then we just make use of the form of $z^{gauss}_w(x)$ to get\n",
        "$$\n",
        "z_{\\omega}^{softmax}(x_i)=\\exp \\left(\\frac{\\|x_i\\|^{2}}{2}\\right)\\left[\\begin{array}{c}\n",
        "\\cos \\left(\\omega^{\\top} x_i\\right) \\\\\n",
        "\\sin \\left(\\omega^{\\top} x_i\\right)\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "</font>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "17RUc3ANFc3w"
      },
      "source": [
        "In the performer paper they show that they can do better than these _Random Fourier Features_ by:\n",
        "\n",
        "1. Using Positive Random Features, which avoid negative values in the case where kernel outputs approach 0 (i.e. small, anti-parallel vectors) - this makes training more stable:$$\n",
        "z_{\\omega}^{\\text {positive }}(x)=\\exp \\left(-\\frac{\\|x\\|^{2}}{2}\\right)\\left[\\exp \\left(\\omega^{\\top} x\\right)\\right]\n",
        "$$\n",
        "\n",
        "2. Ensuring that random samples of $\\omega$ are orthogonal in the $D$-dimensional space, by using the [Gram-Schmidt Orthonormalization procedure](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) (choose some vector, normalize it to get your first dimension, then choose another vector, subtract the components in the first dimension and normalize, and so on).\n",
        "\n",
        "As the Positive Random Features outperform Random Fourier Features, and are easier to implement, we'll use those here (you don't need to worry about orthogonlizing the features, though this does improve the approximation considerably)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DUPPZnHHLCi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# For this question we'll assume that Q and K have the same shape\n",
        "\n",
        "def z_positive(x, omega):\n",
        "    \"\"\"\n",
        "        Given a matrix X (your Q OR K) and sampled freq, returns\n",
        "        positive random features of matching dimensions. \n",
        "        in:\n",
        "            x -     shape [M, d]\n",
        "            omega - shape [rand_dim, d]\n",
        "        out:\n",
        "            z -     shape [rand_dim, d]\n",
        "    \"\"\"\n",
        "\n",
        "    # Solution\n",
        "    coef = np.exp(-np.square(x).sum(axis=-1, keepdims=True) / 2)\n",
        "    product = np.einsum(\"Md,rd->Mr\", x, omega)\n",
        "    z_pos = coef * np.exp(product)\n",
        "    # Solution\n",
        "\n",
        "    return z_pos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FGRyuFsQ-1-I"
      },
      "source": [
        "This can be improved even further by forcing the features to be orthogonal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhM_qrz5Hb_E"
      },
      "outputs": [],
      "source": [
        "# Implement attention using this sampling procedure (NOT BATCHED)\n",
        "def approx_attention(q, k, v, random_dim):\n",
        "    M, d = q.shape\n",
        "    rescale = d ** -0.25              # to normalize before multiplication\n",
        "    \n",
        "    # Generate Random Features - i.i.d. gaussian features\n",
        "    # Solution\n",
        "    omega = np.random.randn(random_dim, d)   \n",
        "    # Solution      \n",
        "\n",
        "    q_prime = z_positive(q * rescale, omega) # apply feature map z to Q\n",
        "    k_prime = z_positive(k * rescale, omega) # apply feature map z to K\n",
        "\n",
        "    # Perform Attention Operation using these, as described at start of Part\n",
        "    A = None\n",
        "    D_inv = None\n",
        "    attn_scores = None\n",
        "\n",
        "    # Solution\n",
        "    # The order of operations is changed for efficiency\n",
        "    A = q_prime @ k_prime.T\n",
        "    D_inv = np.diag(1 / A.sum(-1))\n",
        "    attn_scores =  D_inv @ (A @ v)\n",
        "    # Solution\n",
        "\n",
        "    return attn_scores\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9oWbwvOCXM"
      },
      "source": [
        "We'll now use the code provided below to see how the error between our approximate attention varies from the determinstic one equivalent to that which you implemented in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tto1A4WK6e5c"
      },
      "outputs": [],
      "source": [
        "# Test this against your implementation from flax again and see how error\n",
        "# varies with the number of Random Features\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from flax.nn import dot_product_attention\n",
        "\n",
        "num_iterations = 25 # multiple runs of each to get some statistics\n",
        "\n",
        "# Assume seq length 100, token size 32\n",
        "q, k, v = list(np.random.rand(3,1,500,1,64))\n",
        "\n",
        "# Again, we'll compare to flax's built-in dot_product_attention\n",
        "theirs = np.squeeze(dot_product_attention(q, k, v, deterministic=True))\n",
        "\n",
        "\n",
        "test_range = np.arange(5,150,5)\n",
        "diffs_mean = np.zeros(len(test_range))\n",
        "diffs_std = np.zeros(len(test_range))\n",
        "for i, random_features in enumerate(test_range):\n",
        "    diff = []\n",
        "    for iteration in range(num_iterations):\n",
        "        ours = approx_attention(*map(np.squeeze, (q, k, v)), random_features)\n",
        "        diff.append(np.mean((ours - theirs) **2))\n",
        "\n",
        "    diffs_mean[i] = np.mean(diff)\n",
        "    diffs_std[i] = np.std(diff)\n",
        "    \n",
        "plt.plot(test_range, diffs_mean)\n",
        "plt.fill_between(test_range, diffs_mean-diffs_std, diffs_mean+diffs_std, alpha=0.5)\n",
        "plt.xlabel(\"Number of Random Features\")\n",
        "plt.ylabel(\"MSE\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FFL3JZdlNux7"
      },
      "source": [
        "You should observe a notable improvement when increasing the number of random features for the first 50 dimensions. Beyond that, we get significantly dimishing turns without orthonormalization. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UweHNh6Iye7G"
      },
      "source": [
        "## Performer Complexity analysis\n",
        "Given the form of the softmax attention with sampling derived above (FAVOR+), state the complexity of the operation.\n",
        "\n",
        "Assume that the number of rows in $K$, $Q$ and $V$ all equal the sequence length $L$. Denote the dimension of the positive random features as $m$, and the token dimension as $d$.\n",
        "\n",
        "Recall that vanilla Dot-Product softmax attention has a complexity of $$\\mathcal{O}(L^2d)$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Huyqno4IKNDr"
      },
      "source": [
        "**Your Answer**\n",
        "<font color='green'>\n",
        "$$\\mathcal{O}(Lmd)$$\n",
        "</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmcjX2t6HVv"
      },
      "source": [
        "If you want to double check your complexity, you might try varying the input sequence length (For fixed token dimension, and sampling dimension) and seeing how the execution time varies as a function of $L$. Better still, compare this to your original implementation from Part 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irmLxeXsHvOe"
      },
      "outputs": [],
      "source": [
        "# Some pseudo-oey skeleton code \n",
        "import time\n",
        "\n",
        "random_dim_size = 100\n",
        "test_iterations = 100 # Multiple function calls to get decent statistics\n",
        "\n",
        "attn_funcs = {'slow': ...,\n",
        "              'fast': approx_attention}\n",
        "timings = {'slow': [],\n",
        "           'fast': []}\n",
        "\n",
        "seq_lengths = arange(...)\n",
        "\n",
        "for func_name, func in attn_funcs.items():\n",
        "\n",
        "    for seq_length in seq_lengths:\n",
        "        q, k, v = random... # initialise randomly, with correct sequence length\n",
        "        inputs = (q,k,v) if func_name == 'slow' else (q,k,v,random_dim_size)\n",
        "\n",
        "        start_time = time... # get current time\n",
        "        \n",
        "        for iteration in range(test_iterations): \n",
        "            _ = func(*inputs)\n",
        "        \n",
        "        time_taken = (time... - start_time) / test_iterations\n",
        "\n",
        "        timings[func_name].append(time_taken)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIScs34gG72c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for func_name, timings in timings.items():\n",
        "    plt.plot(seq_lengths, timings, label=func_name)\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "apCLQ6UjEa3B"
      },
      "source": [
        " <a name=\"refs\"></a>\n",
        "# Further Reading\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aBxyWEVFIWo8"
      },
      "source": [
        "## Attention\n",
        "1. Matt Kelcey's talk: [The Map Interpretation of Attention](https://www.youtube.com/watch?v=7wMQgveLiQ4&t=996s)\n",
        "2. Jay Alammar's blog post: [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
        "2. Lilian Weng's blog post: [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
        "3. Vincent Warmerdam's videos: 1 and 2 of the playlist, [Attention is all you Need](https://www.youtube.com/playlist?list=PLvaXXemMsV5CL2DNYTGtdmwyIr0PIz-S7)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GpsQYEGxE2FA"
      },
      "source": [
        "## Slot-Attention \n",
        "* Paper by Locatello et al.: [Object-Centric Learning with Slot Attention](https://arxiv.org/abs/2006.15055)\n",
        "* Thomas Kipf's ICML Talk: [Attentive Grouping and GNNs for Object-Centric Learning](https://slideslive.com/38930703/attentive-grouping-and-gnns-for-objectcentric-learning?ref=speaker-22634-latest ))\n",
        "* Yannic Kilcher's Video on the paper: [Object-Centric Learning with Slot Attention](https://www.youtube.com/watch?v=DYBmD88vpiA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jB8QvHvXImOW"
      },
      "source": [
        "## Transformers\n",
        "1. The original paper by Vaswani et al. : [Attention is All you Need](https://arxiv.org/abs/1706.03762)\n",
        "2. Jay Alammar's blog post: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "3. Alexander Rush's blog post: [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention)\n",
        "4. Amirhossein Kazemnejad's blog post: [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
        "5. Vincent Warmerdam's videos: 3 and 4 of the playlist, [Attention is all you Need](https://www.youtube.com/playlist?list=PLvaXXemMsV5CL2DNYTGtdmwyIr0PIz-S7)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UaQqJyYiIozo"
      },
      "source": [
        "## Performers\n",
        "1. The Google Performer: [Paper](https://arxiv.org/abs/2009.14794) and [Blog Post](https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html)\n",
        "2. Teddy Koker's blog post: [Performers: The Kernel Trick, Random Fourier Features, and Attention](https://teddykoker.com/2020/11/performers/)\n",
        "3. Yannic Kilcher's Video on the paper: [Rethinking Attention with Performers (Paper Explained)](https://www.youtube.com/watch?v=xJrKIPwVwGM&feature=youtu.be)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial 7 Solution.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
