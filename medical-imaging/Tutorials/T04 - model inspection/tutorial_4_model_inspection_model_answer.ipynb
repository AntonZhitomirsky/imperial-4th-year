{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore dimensionality reduction techniques for inspecting the feature representations of different models. So far, we have primarily focused on performance comparisons of different models trained for specific tasks such image classification or semantic segmentation. Model inspection can be provide insights about the characteristics of the learned representations.\n",
    "\n",
    "**Objective:** Use dimensionality reduction techniques to visualise feature representations of deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google Colab uncomment the following line to install PyTorch Lightning\n",
    "# ! pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 32, num_workers: int = 4, transform = transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform        \n",
    "\n",
    "        self.test_set = MNIST(self.data_dir, train=False, transform=self.transform, download=True)\n",
    "        dev_set = MNIST(self.data_dir, train=True, transform=self.transform, download=True)\n",
    "        self.train_set, self.val_set = random_split(dev_set, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for implementing the model and its training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(LightningModule):\n",
    "    def __init__(self, input_dim: tuple[int, int] = (28,28), output_dim: int = 10, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # LeNet\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            )        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, self.output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first pass x through the conv layers\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # then pass linearised x through the fully connected layers\n",
    "        return self.fc(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)        \n",
    "        acc = accuracy(preds, y, task='multiclass', num_classes=self.output_dim)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        if batch_idx == 0:\n",
    "            grid = torchvision.utils.make_grid(batch[0][0:16, ...], nrow=4, normalize=True)\n",
    "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the LeNet-like model previously trained for MNIST image classification.\n",
    "\n",
    "**Task:** Add the path to your pre-trained MNIST classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "\n",
    "data = MNISTDataModule(data_dir='./data', batch_size=32)\n",
    "\n",
    "# model_dir = '<path_to_model_checkpoint>' # for example: './lightning_logs/classification/mnist-lenet/version_0/checkpoints/epoch=5-step=10314.ckpt'\n",
    "model_dir = '../lightning_logs/classification/mnist-lenet/version_0/checkpoints/epoch=5-step=10314.ckpt'\n",
    "model = ImageClassifier.load_from_checkpoint(model_dir, input_dim=(28,28), output_dim=10)\n",
    "\n",
    "trainer = Trainer() # dummy trainer for running test() on the loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the pre-trained model on the test data and confirm the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the feature representations (also called embeddings) for test images, we need to modify the model. We want to obtain the feature vectors coming out of the convolutional part which would normally be passed on to the classification head (typically one or more fully connected layers) to make final predictions.\n",
    "\n",
    "In the modified model below (derived from the original model), we already modified the test step to collate feature embeddings in a list.\n",
    "\n",
    "**Task:** Implement the `get_embedding` function which should return the feature embeddings for a batch of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierEmbeddings(ImageClassifier):\n",
    "    def __init__(self, input_dim: tuple[int, int] = (28,28), output_dim: int = 10, learning_rate: float = 0.001):\n",
    "        super().__init__(input_dim, output_dim, learning_rate)       \n",
    "        self.embeddings = [] # list where we still store the embeddings\n",
    "\n",
    "    def get_embedding(self, batch):\n",
    "        x, _ = batch\n",
    "        x = self.conv(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "    \n",
    "    def on_test_start(self):\n",
    "        self.embeddings = [] # clear the list of embeddings at the start of testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        emb = self.get_embedding(batch)\n",
    "        self.embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified model we can now run the test routine again. After some reformatting and data conversion, we obtain a 2D array of stacked feature vectors (one per test image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_modified = ImageClassifierEmbeddings.load_from_checkpoint(model_dir, input_dim=(28,28), output_dim=10)\n",
    "trainer.test(model=model_modified, datamodule=data)\n",
    "embeddings = torch.cat(model_modified.embeddings, dim=0).cpu().numpy()\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LeNet-like CNN above, we obtain a 256-dimensional feature vector per MNIST image, as the last convolutional layer outputs 16 feature maps of size 4 x 4.\n",
    "\n",
    "In order to visualise and inspect these high-dimensional features, we can apply dimensionality reduction techniques such as [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "We first create a pandas dataframe that we will use later for ease of visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([data.test_set[i][1] for i in range(0,len(data.test_set))]).astype(np.uint8)\n",
    "df = pd.DataFrame(labels, columns=['class_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the PCA implementation of scikit-learn. Settging `n_components=0.95` will give us a PCA embedding that preserves 95% of the variance of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=0.95, whiten=False)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "print(embeddings_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA has reduced the dimensionality of the feature vectors to 96. This is still too large for visualisations, but given that PCA will order the dimensions by \"importance\" (that is how much variance each dimension captures), we can just retain a few PCA dimensions for later visualisation which may capture the most important information.\n",
    "\n",
    "Let us store the first four dimensions of the PCA embeddings in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['features - PCA 1'] = embeddings_pca[:,0]\n",
    "df['features - PCA 2'] = embeddings_pca[:,1]\n",
    "df['features - PCA 3'] = embeddings_pca[:,2]\n",
    "df['features - PCA 4'] = embeddings_pca[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the popular t-SNE algorithm for obtaining a 2D dimensional embedding for high-dimensional input data. Here we can use the PCA embeddings as the input for t-SNE (which has been shown to be beneficial for obtaining more meaningful t-SNE embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tsne = TSNE(n_components=2, init='random', learning_rate='auto').fit_transform(embeddings_pca)\n",
    "\n",
    "print(embeddings_tsne.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us store the two t-SNE dimensions in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['features - t-SNE 1'] = embeddings_tsne[:,0]\n",
    "df['features - t-SNE 2'] = embeddings_tsne[:,1]\n",
    "\n",
    "df.head() # showing the first five entries in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe now contains all the information we need to visualise the feature embeddings. However, in order to better compare and appreciate the discriminative nature of these learned feature representations, we can also perform the same pipeline of dimensionality reduction via PCA and t-SNE to the input images.\n",
    "\n",
    "To this end, we treat the images as 784-dimensional feature vectors (28 x 28 pixels = 784) where each feature corresponds to the pixel's intensity. Remember, this was the input to the logistic regression-like model we have seen in earlier tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array([data.test_set[i][0] for i in range(0,len(data.test_set))])\n",
    "images = images.reshape(images.shape[0], -1) # linearize the 28x28 MNIST input images\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=0.95, whiten=False)\n",
    "images_pca = pca.fit_transform(images)\n",
    "\n",
    "df['images - PCA 1'] = images_pca[:,0]\n",
    "df['images - PCA 2'] = images_pca[:,1]\n",
    "df['images - PCA 3'] = images_pca[:,2]\n",
    "df['images - PCA 4'] = images_pca[:,3]\n",
    "\n",
    "print(images_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_tsne = TSNE(n_components=2, init='random', learning_rate='auto').fit_transform(images_pca)\n",
    "\n",
    "df['images - t-SNE 1'] = images_tsne[:,0]\n",
    "df['images - t-SNE 2'] = images_tsne[:,1]\n",
    "\n",
    "print(images_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualisation, we have quite a lot of test images (10,000 for MNIST), so let's take a random sample with just 1,000 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the distribution of feature embeddings via scatter plots with colour-coded class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "style = 'o'\n",
    "markersize = 40\n",
    "color_palette = 'tab10'\n",
    "kind = 'scatter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first and second PCA dimensions of the feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "ax = sns.scatterplot(data=sample, x='features - PCA 1', y='features - PCA 2', hue='class_label', alpha=alpha, marker=style, s=markersize, palette=color_palette)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this to the first and second PCA dimensions of the image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "ax = sns.scatterplot(data=sample, x='images - PCA 1', y='images - PCA 2', hue='class_label', alpha=alpha, marker=style, s=markersize, palette=color_palette)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do similar for the third and fourth PCA dimension. Feel free to try it out.\n",
    "\n",
    "But let us now plot the t-SNE dimensions, first for the feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "ax = sns.scatterplot(data=sample, x='features - t-SNE 1', y='features - t-SNE 2', hue='class_label', alpha=alpha, marker=style, s=markersize, palette=color_palette)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's compare this with the t-SNE dimensions of the image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "ax = sns.scatterplot(data=sample, x='images - t-SNE 1', y='images - t-SNE 2', hue='class_label', alpha=alpha, marker=style, s=markersize, palette=color_palette)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the learned feature embeddings yield a better and more consistent separation of digit classes compared to the image embeddings. However, we can see that even the image embeddings nicely separate the digit classes. This is not too suprising, given that we obtained high accuracy for MNIST digit classification with a simple linear model (the logistic regression-like model implemented as a single-layer network). This will be very different for more difficult image classification tasks where the class separation will be less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualisation\n",
    "\n",
    "The following visualisation allows you to interact with the scatter plot and display the corresponding images. This is a great way to better understand failure cases. Look out for images that are located in the wrong digit cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from matplotlib import cm\n",
    "from ipywidgets import Output, HBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_hex(rgb):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(rgb[0], rgb[1], rgb[2])\n",
    "\n",
    "color = cm.tab10(np.linspace(0, 1, 10))\n",
    "colorlist = [(np.array(mpl.colors.to_rgb(c))*255).astype(int).tolist() for c in color]*10\n",
    "\n",
    "colors = [rgb_to_hex(colorlist[label]) for label in sample.class_label.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'features - t-SNE 1'\n",
    "y = 'features - t-SNE 2'\n",
    "\n",
    "out = Output()\n",
    "@out.capture(clear_output=True)\n",
    "def handle_click(trace, points, state):\n",
    "    idx = sample.index.values[points.point_inds[0]]\n",
    "    img = images[idx, :]\n",
    "    \n",
    "    s = [8] * len(sample)\n",
    "    for i in points.point_inds:\n",
    "        s[i] = 16\n",
    "    with fig.batch_update():\n",
    "        scatter.marker.size = s\n",
    "\n",
    "    f, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "    ax.imshow(img.reshape((28,28)), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    plt.show(f)\n",
    "    \n",
    "fig = go.FigureWidget(px.scatter(sample, x=x, y=y, template='plotly_white', hover_data={'class_label': True, x:False, y:False}))\n",
    "fig.update_layout(width=600, height=600)\n",
    "scatter = fig.data[0]\n",
    "scatter.on_click(handle_click)\n",
    "scatter.marker.size = [8] * len(sample)\n",
    "scatter.marker.color = colors\n",
    "\n",
    "HBox([fig, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform a similar model inspection for one of the chest X-ray classification models from the coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mli-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
