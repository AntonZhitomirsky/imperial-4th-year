{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will develop a CNN model for image segmentation. The structure of the code is similar to the one for image classification, but with some specific changes due to the different task. This time we are using real-world clinical data from a chest radiography dataset called JSRT. We consider a four class image segmentation taks involving the segmentation of the heart and the lungs.\n",
    "\n",
    "**Objective:** Implement a fully convolutional neural network such as U-net for semantic segmentation of chest radiography images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google Colab uncomment the following line to install PyTorch Lightning\n",
    "# ! pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from torchmetrics.functional import dice\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download and extract the JSRT chest radiography dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/JSRT.zip\n",
    "! unzip JSRT.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise a random image with its segmentation labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread('data/JSRT/images/JPCLN001.png')\n",
    "labelmap = imread('data/JSRT/masks/JPCLN001.png')\n",
    "\n",
    "f, ax = plt.subplots(1,2, figsize=(10, 10))\n",
    "\n",
    "ax[0].imshow(image, cmap=matplotlib.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('image')\n",
    "\n",
    "ax[1].imshow(labelmap, cmap=matplotlib.cm.gray)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('labelmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a custom [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) that loads and processes the JSRT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSRTDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, data_dir: str, augmentation: bool = False):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.data_dir = data_dir        \n",
    "        self.do_augment = augmentation\n",
    "\n",
    "        # photometric data augmentation\n",
    "        self.photometric_augment = transforms.Compose([\n",
    "            transforms.RandomApply(transforms=[transforms.ColorJitter(brightness=0.2, contrast=0.2)], p=0.5),\n",
    "        ])\n",
    "\n",
    "        # geometric data augmentation\n",
    "        self.geometric_augment = transforms.Compose([\n",
    "            transforms.RandomApply(transforms=[transforms.RandomAffine(degrees=5, scale=(0.9, 1.1), interpolation=transforms.InterpolationMode.NEAREST)], p=0.5),\n",
    "        ])\n",
    "\n",
    "        # collect samples (file paths) from dataset\n",
    "        self.samples = []\n",
    "        for idx, _ in enumerate(tqdm(range(len(self.data)), desc='Loading Data')):\n",
    "            img_path = os.path.join(self.data_dir, 'images', self.data.loc[idx, 'study_id'])\n",
    "            lab_path = os.path.join(self.data_dir, 'masks', self.data.loc[idx, 'study_id'])\n",
    "\n",
    "            sample = {'image_path': img_path, 'labelmap_path': lab_path}\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = self.get_sample(item)\n",
    "\n",
    "        # conver to torch tensors and add batch dimension\n",
    "        image = torch.from_numpy(sample['image']).unsqueeze(0)\n",
    "        labelmap = torch.from_numpy(sample['labelmap']).unsqueeze(0)\n",
    "\n",
    "        # apply data augmentation\n",
    "        if self.do_augment:\n",
    "            image = self.photometric_augment(image.type(torch.ByteTensor)).type(torch.FloatTensor)\n",
    "            \n",
    "            # Stack the image and mask together so they get the same geometric transformations\n",
    "            stacked = torch.cat([image, labelmap], dim=0)  # shape=(2xHxW)\n",
    "            stacked = self.geometric_augment(stacked)\n",
    "\n",
    "            # Split them back up again and convert labelmap back to datatype long\n",
    "            image, labelmap = torch.chunk(stacked, chunks=2, dim=0)\n",
    "            labelmap = labelmap.type(torch.LongTensor)\n",
    "\n",
    "        # normalize image intensities to [0,1]\n",
    "        image /= 255\n",
    "\n",
    "        return {'image': image, 'labelmap': labelmap}\n",
    "\n",
    "    def get_sample(self, item):\n",
    "        sample = self.samples[item]\n",
    "        \n",
    "        # read image and labelmap from disk\n",
    "        image = imread(sample['image_path']).astype(np.float32)\n",
    "        labelmap = imread(sample['labelmap_path']).astype(np.int64)\n",
    "\n",
    "        # convert labelmap to consecutive labels\n",
    "        labelmap[labelmap==250] = 1 # heart\n",
    "        labelmap[labelmap==200] = 2 # left lung\n",
    "        labelmap[labelmap==150] = 3 # right lung\n",
    "        labelmap[labelmap>3] = 0 # background\n",
    "\n",
    "        return {'image': image, 'labelmap': labelmap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling the JSRT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSRTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data/JSRT', batch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.data = pd.read_csv(os.path.join(self.data_dir,'jsrt_metadata.csv'))\n",
    "        dev, self.test_data = train_test_split(self.data, test_size=0.2)\n",
    "        self.train_data, self.val_data = train_test_split(dev, test_size=0.05)\n",
    "        \n",
    "        self.train_set = JSRTDataset(self.train_data, self.data_dir, augmentation=True)\n",
    "        self.val_set = JSRTDataset(self.val_data, self.data_dir, augmentation=False)\n",
    "        self.test_set = JSRTDataset(self.test_data, self.data_dir, augmentation=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for implementing the model and its training and testing steps.\n",
    "\n",
    "**Task:** Replace the single-layer network with a U-net-like CNN architecture. You need to add the model layers and change the forward pass accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmenter(LightningModule):\n",
    "    def __init__(self, output_dim: int, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.test_probmaps = []\n",
    "        self.test_predmaps = []\n",
    "\n",
    "        # single-layer network (regression-like)\n",
    "        self.conv = nn.Conv2d(1, self.output_dim, kernel_size=7, padding=3)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.conv(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        x, y = batch['image'], batch['labelmap']\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y.squeeze())\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        dsc = dice(preds, y.squeeze(), average='macro', num_classes=self.output_dim, ignore_index=0)\n",
    "        \n",
    "        return loss, dsc, probs, preds\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, dsc, probs, preds = self.process_batch(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_dice\", dsc, prog_bar=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            grid = torchvision.utils.make_grid(batch['image'][0:4, ...], nrow=2, normalize=True)\n",
    "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
    "\n",
    "            grid = torchvision.utils.make_grid(batch['labelmap'][0:4, ...].type('torch.FloatTensor'), nrow=2, normalize=True)\n",
    "            self.logger.experiment.add_image('train_labelmaps', grid, self.global_step)\n",
    "\n",
    "            grid = torchvision.utils.make_grid(probs[0:4, 1:4, ...], nrow=2, normalize=True)\n",
    "            self.logger.experiment.add_image('train_probmaps', grid, self.global_step)\n",
    "        \n",
    "            grid = torchvision.utils.make_grid(preds[0:4, ...].unsqueeze(1).type('torch.FloatTensor'), nrow=2, normalize=True)\n",
    "            self.logger.experiment.add_image('train_predmaps', grid, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, dsc, _, _ = self.process_batch(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_dice\", dsc, prog_bar=True)\n",
    "\n",
    "    def on_test_start(self):\n",
    "        self.test_probmaps = []\n",
    "        self.test_predmaps = []\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, dsc, probs, preds = self.process_batch(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_dice\", dsc)\n",
    "        self.test_probmaps.append(probs)\n",
    "        self.test_predmaps.append(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the PyTorch Lightning [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "\n",
    "data = JSRTDataModule(data_dir='./data/JSRT', batch_size=5)\n",
    "\n",
    "model = ImageSegmenter(output_dim=4, learning_rate=0.001)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=5,\n",
    "    logger=TensorBoardLogger(save_dir='./lightning_logs/segmentation/', name='jsrt-single-conv'),\n",
    "    callbacks=[ModelCheckpoint(monitor=\"val_loss\", mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
    ")\n",
    "trainer.fit(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model with the best checkpoint on the validation data and report the segmentation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model with the best checkpoint on the test data and report the segmentation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the segmentation result for a random test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = data.test_set[0]\n",
    "image = test_sample['image'].squeeze()\n",
    "labelmap = test_sample['labelmap'].squeeze()\n",
    "probmap = np.moveaxis(model.test_probmaps[0][0,1:4,...].cpu().numpy(), 0, -1)\n",
    "predmap = model.test_predmaps[0][0,...].cpu().numpy()\n",
    "\n",
    "f, ax = plt.subplots(1,4, figsize=(15, 15))\n",
    "\n",
    "ax[0].imshow(image, cmap=matplotlib.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('image')\n",
    "\n",
    "ax[1].imshow(labelmap, cmap=matplotlib.cm.gray)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('labelmap')\n",
    "\n",
    "ax[2].imshow(predmap, cmap=matplotlib.cm.gray)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('prediction')\n",
    "\n",
    "ax[3].imshow(probmap, cmap=matplotlib.cm.bwr)\n",
    "ax[3].axis('off')\n",
    "ax[3].set_title('probability map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir './lightning_logs/segmentation/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mli-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
