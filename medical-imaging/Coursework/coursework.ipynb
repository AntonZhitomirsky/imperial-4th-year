{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlp4sb-8i3i"
      },
      "source": [
        "# Coursework: Self-supervised learning\n",
        "\n",
        "**Submission:** You are asked to submit two versions of your notebook:\n",
        "1. You should submit the raw notebook in `.ipynb` format with *all outputs cleared*. Please name your file `coursework.ipynb`.\n",
        "2. Additionally, you will be asked to submit an exported version of your notebook in `.pdf` format, with *all outputs included*. We will primarily use this version for marking, but we will use the raw notebook to check for correct implementations. Please name this file `coursework_export.pdf`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxpFCs1SSOGF"
      },
      "source": [
        "## Your details\n",
        "\n",
        "Authors: **Anton Zhitomirsky**\n",
        "\n",
        "DoC alias: **az620**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4nr-pFhSOGF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install lightning medmnist tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJAj8P5r8i3m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "# from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
        "from torchmetrics.functional import auroc\n",
        "from PIL import Image\n",
        "from medmnist.info import INFO\n",
        "from medmnist.dataset import MedMNIST\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7v3hY78i3n"
      },
      "source": [
        "## **Part A:** Implement a dataset suitable for contrastive learning.\n",
        "\n",
        "Dataset: [MedMNIST Pneumonia](https://medmnist.com/) (chest X-ray images downsampled to 28 $\\times$ 28 pixels), with binary labels indicating the presence of Pneumonia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EAXIOzSOGJ"
      },
      "source": [
        "### **Task A-1:** Complete the dataset implementation.\n",
        "\n",
        "- implement a dataset class `SimCLRPneumoniaMNISTDataset`\n",
        "- suitable for training a self-supervised model with a contrastive objective\n",
        "- For each sample, your dataset class should return two 'views' of the corresponding image, forming the positive pairs for contrastive learning.\n",
        "- It is up to you to design suitable augmentation pipeline for generating these views\n",
        "- provide a short description\n",
        "\n",
        "Once you have implemented your dataset class, you are asked to run the provided visualisation code to visualise one batch of your training dataloader.\n",
        "\n",
        "*Note:* You can use the same data augmentation pipeline for training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WpnXKjrm0bL"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "class SimCLRPneumoniaMNISTDataset(MedMNIST):\n",
        "    def __init__(self, split = 'train'):\n",
        "        \"\"\"Dataset class for PneumoniaMNIST.\n",
        "        \n",
        "        The provided init function will automatically download the necessary\n",
        "        files at the first class initialization.\n",
        "\n",
        "        Args:\n",
        "            split (str, optional): select subset of the data: 'train', 'val' or 'test'. Defaults to 'train'.\n",
        "        \"\"\"\n",
        "\n",
        "        self.flag = \"pneumoniamnist\"\n",
        "        self.size = 28\n",
        "        self.size_flag = \"\"\n",
        "        self.root = './data/coursework/'\n",
        "        self.info = INFO[self.flag]\n",
        "        self.download()\n",
        "\n",
        "        npz_file = np.load(os.path.join(self.root, \"pneumoniamnist.npz\"))\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Load all the images\n",
        "        assert self.split in ['train','val','test']\n",
        "\n",
        "        self.imgs = npz_file[f'{self.split}_images']\n",
        "        self.labels = npz_file[f'{self.split}_labels']\n",
        "\n",
        "        \"\"\"\n",
        "        The data augmentation follows a similar style as in the original paper's default settings with some minor adjustments for the training dataset that was given. The steps for the data augmentation are provided below with reasoning:\n",
        "\n",
        "            - random crop and resize: \n",
        "                The original paper uses images from the ImageNet, with avg size of approx 400x400. They performed aggressive random crops (from 0.08 <= x <= 1 of original) in area and a random aspect ratio (3/4 <= x <= 4/3) of the original aspect ratio made. Additionally a random crop + resize is followed by a random horizontal flip with 50\\% probability.\n",
        "\n",
        "                For this coursework, we have pneumonia images that have been scaled down to a 28x28 resolution. I am inspired by the paper to perform a random aggressive crop of 1/16 <= x <= 1 of the original area (at the extreme, this gives a 7x7 area to process which BY EYE seems aggressive enough without loosing too much detail). Further default parameters have been kept as is.\n",
        "\n",
        "            - random color-discoloration:\n",
        "                The original paper uses random distortion composed by color jittering and color dropping. However, our images come in black-and-white. Therefore, for this coursework we only utilize certain aspects of the color jittering with their suggested strength parameter 0.8*s. For our purpose, hue is irrelevant as we want to keep the model to learn only grey-scale features.\n",
        "\n",
        "            - gaussian blur:\n",
        "                In the paper, they blur the image 50% of the time using a Gaussian kernel. We randomly sample \\sigma âˆˆ [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Documentation: https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "\n",
        "        random_crop = v2.RandomResizedCrop(size=self.size, scale=(0.0625, 1), ratio=(3/4, 4/3))\n",
        "        \n",
        "        # Documentation: http://pytorch.org/vision/master/generated/torchvision.transforms.RandomHorizontalFlip.html\n",
        "\n",
        "        random_h_flip = v2.RandomHorizontalFlip(0.5)\n",
        "\n",
        "        # Documentation: https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html\n",
        "        # Why no hue: We want to keep the model strictly learning about black-and-white features from the black-and-white scan. https://color-wheel-artist.com/hue/\n",
        "        # Why no saturation: Increasing or decreasing saturation on a black and white image typically has no effect https://www.imaginated.com/blog/what-is-saturation/#What_is_Color_Saturation\n",
        "\n",
        "        strength = 1\n",
        "        color_jitter = v2.ColorJitter(brightness=0.8*strength, contrast=0.8*strength)\n",
        "\n",
        "        # Documentation: https://pytorch.org/vision/master/generated/torchvision.transforms.GaussianBlur.html\n",
        "        # Custom Transforms: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
        "\n",
        "        class RandomGaussianBlur(object):\n",
        "            def __init__(self, size, p=0.5):\n",
        "                \"\"\"Performs a GaussianBlur with a 50% chance. The unsuccessful times, the image is left as is.\n",
        "\n",
        "                Args:\n",
        "                    p (float, optional): Probability of performing a gaussian blur. Defaults to 0.5.\n",
        "                \"\"\"\n",
        "\n",
        "                self.size = size\n",
        "                self.p = 0.5\n",
        "\n",
        "            def __call__(self, x):\n",
        "                \"\"\"Stochastically performs a gaussian blur on the input defined by probability self.p\n",
        "\n",
        "                Args:\n",
        "                    x (_type_): _description_\n",
        "                \"\"\"\n",
        "                \n",
        "                if (torch.rand(1).item() < self.p):\n",
        "                    # Gaussian filter size should be odd and positive\n",
        "                    kernel_size = int(0.1*self.size)\n",
        "                    kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size\n",
        "                    gaussian_blur = v2.GaussianBlur(kernel_size=kernel_size, sigma=(0.1, 2.0))\n",
        "                    return gaussian_blur(x)\n",
        "                else:\n",
        "                    return x        \n",
        "\n",
        "        gaussian_blur = RandomGaussianBlur(size=self.size, p=0.5)\n",
        "\n",
        "        # Define the data augmentation pipeline.\n",
        "\n",
        "        self.augmentation_pipeline = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            random_crop,\n",
        "            random_h_flip,\n",
        "            color_jitter,\n",
        "            gaussian_blur\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TASK: Fill in the blanks such that you return two tensors\n",
        "        # of shape [1, 28, 28], img_view1 and img_view2, representing two augmented view of the images.\n",
        "        \n",
        "        # Get the image at the indexed position\n",
        "        x = np.expand_dims(self.imgs[index], axis=0)\n",
        "\n",
        "        x = torch.from_numpy(x)\n",
        "\n",
        "        return self.augmentation_pipeline(x), self.augmentation_pipeline(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqYUOutSOGK"
      },
      "source": [
        "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling your PneumoniaMNIST dataset. You do not need to make any modifications to the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQTVOXwESOGL"
      },
      "outputs": [],
      "source": [
        "class SimCLRPneumoniaMNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, batch_size: int = 8):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_set = SimCLRPneumoniaMNISTDataset(split='train')\n",
        "        self.val_set = SimCLRPneumoniaMNISTDataset(split='val')\n",
        "        self.test_set = SimCLRPneumoniaMNISTDataset(split='test')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxDF8J8CSOGL"
      },
      "source": [
        "#### **Check** dataset implementation.\n",
        "\n",
        "Run the below cell to visualise a batch of your training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbV-xhDitmrR"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "# Initialise data module\n",
        "datamodule = SimCLRPneumoniaMNISTDataModule()\n",
        "# Get train dataloader\n",
        "train_dataloader = datamodule.train_dataloader()\n",
        "# Get first batch\n",
        "batch = next(iter(train_dataloader))\n",
        "# Visualise the images\n",
        "view1, view2 = batch\n",
        "f, ax = plt.subplots(2, 8, figsize=(12,4))\n",
        "for i in range(8):\n",
        "  ax[0,i].imshow(view1[i, 0], cmap='gray')\n",
        "  ax[1,i].imshow(view2[i, 0], cmap='gray')\n",
        "  ax[0,i].set_title('view 1')\n",
        "  ax[1,i].set_title('view 2')\n",
        "  ax[0, i].axis(\"off\")\n",
        "  ax[1, i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9dmJ2CV8i3o"
      },
      "source": [
        "## **Part B:** Implement the SimCLR loss and training step.\n",
        "\n",
        "In this part, we ask you to:\n",
        "1. Implement the SimCLR loss function, as per the equation in the lecture notes (and the [original paper](https://arxiv.org/abs/2002.05709)).\n",
        "2. Once you have implemented the loss, implement the training step function in the provided LightningModule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg3W4Mh8anKn"
      },
      "source": [
        "### **Task B-1:** SimCLR loss function.\n",
        "\n",
        "For the implementation of the SimCLR loss, you should follow the 'recipe' from the lecture slides. We provide a code skeleton to get you started. Fill in all the blanks.\n",
        "\n",
        "*Hint:* In PyTorch, to compute scalar products (also called dot products) between many elements efficiently, note that for two batches of $d$-dimensional feature vectors $v1$ and $v2$ of size $[N, d]$ (with $N$ being the batch size) computing the matrix multiplication `torch.mm(v1, v2.t())` returns a matrix $S$ of size $[N, N]$ where each element $S[i, j]$ is the scalar product of $v1_i$ and $v2_j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjS5kMqsFPZg"
      },
      "outputs": [],
      "source": [
        "def simclr_loss(embedding_view1, embedding_view2, tau = 1.0):\n",
        "  \"\"\"This function implements the SimCLR loss function as described in the original paper.\n",
        "  See lecture notes for formulas.\n",
        "\n",
        "  It takes as input the embeddings from both views and returns the loss value for that batch.\n",
        "\n",
        "  Args:\n",
        "      embedding_view1 (torch.tensor): shape [batch_size, embedding_dimension]\n",
        "      embedding_view2 (torch.tensor): shape [batch_size, embedding_dimension]\n",
        "      tau (float, optional): temperature parameter. Defaults to 1.0.\n",
        "\n",
        "  Returns:\n",
        "      torch.tensor: shape 1\n",
        "  \"\"\"\n",
        "\n",
        "  # Step 1: normalize the embeddings with L_2 normalization over the embedding_dimension\n",
        "  embedding_view1 = F.normalize(embedding_view1, dim=-1)\n",
        "  embedding_view2 = F.normalize(embedding_view2, dim=-1)\n",
        "\n",
        "  # Step 2: gather all embeddings into one big vector of size [2*N , feature_dim]\n",
        "  embedding_combined = torch.cat((embedding_view1, embedding_view2), dim=0)\n",
        "\n",
        "  # Step 3: compute all possible similarities, should be a matrix of size [2 * N, 2 * N]\n",
        "  # all_similarities[i,j] will be the similarity between z_all_views[i] and z_all_views[j].\n",
        "  # We don't need any further division by ||i|| * ||j|| because they are already normalized. \n",
        "  all_similarities = torch.mm(embedding_combined, embedding_combined.T)\n",
        "\n",
        "  # Step 4: Here we want to return a mask of size[2 * N, 2 * N] for which mask[i,j] = 1 if\n",
        "  # z_all_views[i] and z_all_views[j] form a positive pair.\n",
        "  # There should be exactly 2 * N non-zeros elements in this matrix.\n",
        "  mask_self = torch.eye(all_similarities.size(0))\n",
        "  mask_positive_pair = torch.roll(mask_self, shifts=embedding_view1.size(0), dims=1)\n",
        "\n",
        "  # Step 5: self-mask. For computing the denominator term in the loss function,\n",
        "  # we need to sum over all possible similarities except the self-similarity.\n",
        "  # Create a mask of shape [2*N, 2*N] that is 1 for all valid pairs and 0 for all self-pairs (i = j).\n",
        "  mask_negative = 1 - mask_self\n",
        "  \n",
        "  # Step 6: Computing all numerators for the loss function.\n",
        "  # Should be vector of size [2 * N],\n",
        "  # where element is exp(sim(i, j) / t) for each positive pair (i, j).\n",
        "  # Re-use the computed quantities above.\n",
        "\n",
        "  \"\"\"\n",
        "  print(mask_positive_pair.get_device()) # -1\n",
        "  print(all_similarities.get_device())   # 0\n",
        "  \"\"\"\n",
        "\n",
        "  mask_positive_pair = mask_positive_pair.to(embedding_view1.device)\n",
        "  numerator = torch.exp(torch.sum(mask_positive_pair * all_similarities, dim=1)/tau)\n",
        "\n",
        "  # Step 7: Computing all denominators for the loss function.\n",
        "  # Should be a vector of size [2 * N].\n",
        "  # Where each element should be the sum of exp(sim(i,k)/tau) for all k != i.\n",
        "\n",
        "  \"\"\"\n",
        "  print(mask_negative.get_device())    # -1\n",
        "  print(all_similarities.get_device()) # 0\n",
        "  \"\"\"\n",
        "\n",
        "  mask_negative = mask_negative.to(embedding_view1.device)\n",
        "  denominator = torch.sum(mask_negative * torch.exp(all_similarities/tau), dim=1)\n",
        "\n",
        "  # Step 8: Return the final loss values, using the previously computing numerators and denominators.\n",
        "  batch_loss = - torch.log(numerator / denominator)\n",
        "\n",
        "  return torch.mean(batch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHkOZiucO3-D"
      },
      "source": [
        "#### **Check** SimCLR loss function.\n",
        "\n",
        "To check your implementation, please run the following tests. Note that we will also use other tests on different inputs to test your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orohu5xQO3KA"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "seed_everything(33)\n",
        "\n",
        "expected_results = [torch.tensor(1.7518), torch.tensor(1.6376), torch.tensor(4.194),  torch.tensor(4.1754)]\n",
        "for i, (N, feature_dim) in enumerate(zip([3, 3, 33, 33], [5, 125, 5, 125])):\n",
        "  embedding_view1 = torch.rand((N, feature_dim))#.to(device)\n",
        "  embedding_view2 = torch.rand((N, feature_dim))#.to(device)\n",
        "  loss = simclr_loss(embedding_view1.clone(), embedding_view2.clone(), tau=0.5)\n",
        "  print(f\"Expected loss: {expected_results[i]}, Computed loss: {loss}\")\n",
        "  assert torch.isclose(loss, expected_results[i], rtol=1e-3)\n",
        "print(\"Passed all tests successfully !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir0Myj1n-q_G"
      },
      "source": [
        "### **Task B-2:** SimCLR training step.\n",
        "\n",
        "In this next task you are asked to complete the blanks in the provided [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html).\n",
        "\n",
        "We provide the implementation of an image encoder (the CNN backbone that will act as feature extractor). No changes are needed for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u77dpCujSOGO"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(torch.nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.net = models.resnet50(weights=None)\n",
        "        del self.net.fc\n",
        "        self.net.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.net.conv1(x)\n",
        "        x = self.net.bn1(x)\n",
        "        x = self.net.relu(x)\n",
        "        x0 = self.net.maxpool(x)\n",
        "        x1 = self.net.layer1(x0)\n",
        "        x2 = self.net.layer2(x1)\n",
        "        x3 = self.net.layer3(x2)\n",
        "        x4 = self.net.layer4(x3)\n",
        "        x4 = self.net.avgpool(x4)\n",
        "        x4 = torch.flatten(x4, 1)\n",
        "        return x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ih6SONfSOGO"
      },
      "source": [
        "Next, you will need to complete the implementation of the SimCLR model. In order to make the training step work correctly, you will need to implement the `process_batch` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sizIiNm8i3o"
      },
      "outputs": [],
      "source": [
        "class SimCLRModel(LightningModule):\n",
        "    def __init__(self, learning_rate: float = 0.001):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.encoder = ImageEncoder()\n",
        "\n",
        "        self.projector = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2048, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 128),\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def process_batch(self, batch):\n",
        "        # Process the 'left' and 'right' execution graph separately\n",
        "\n",
        "        x1 = self.encoder(batch[0].float())\n",
        "        x1 = self.projector(x1)\n",
        "\n",
        "        x2 = self.encoder(batch[1].float())\n",
        "        x2 = self.projector(x2)\n",
        "\n",
        "        x1 = x1.float()\n",
        "        x2 = x2.float()\n",
        "\n",
        "        loss = simclr_loss(x1, x2)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.process_batch(batch)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        if batch_idx == 0:\n",
        "            grid = torchvision.utils.make_grid(torch.cat((batch[0][0:4, ...], batch[1][0:4, ...]), dim=0), nrow=4, normalize=True)\n",
        "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.process_batch(batch)\n",
        "        self.log('val_loss', loss, prog_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgSl_FnT_H0B"
      },
      "source": [
        "#### **Check** SimCLR training step.\n",
        "\n",
        "Here you can test that your code runs fine by training the model for 5 epochs using the cell below.\n",
        "\n",
        "Report the training and validation loss at the end of 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = SimCLRPneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "model = SimCLRModel()\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/coursework/', name='simclr'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUmT50C8i3p"
      },
      "source": [
        "## **Part C:** Linear probing and model finetuning.\n",
        "\n",
        "Implementation of transfer learning strategies (linear probing and finetuning) for model evaluation.\n",
        "\n",
        "In this part, you are given two different image encoders that were pre-trained with different datasets and training strategies. The objective for this task is to assess the performance of these two encoders in a downstream classification task. For this task, you are asked to implement evaluation routines seen in the lecture: linear probing and model finetuning. The downstream task is the prediction of Pneumonia in the (small) chest X-ray images from the PneumoniaMNIST dataset.\n",
        "\n",
        "This part can be broken down into the following tasks:\n",
        "1. Adapt your PneunomiaMNIST dataset for the image classification task.\n",
        "2. Implement a classification model with a linear layer attached to a pre-trained image encoder.\n",
        "3. For both pre-trained encoders:\n",
        "    - a) Train the classifier on top of the frozen encoder (linear probing)\n",
        "    - b) Finetune the entire model (including the encoder).\n",
        "4. Evaluate all models on the test set, and provide a brief summary (no more than 300 words) with an analysis of your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8eiyVBc_429"
      },
      "source": [
        "### **Task C-1:** Adapt your PneunomiaMNIST dataset for the image classification task.\n",
        "\n",
        "We can base our implementation largely on the `SimCLRPneumoniaMNISTDataset` and adapt it to make it suitable for image classification. Think about a suitable data augmentation pipeline. Check previous tutorials for inspiration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5WXLUOtBpw_"
      },
      "outputs": [],
      "source": [
        "class PneumoniaMNISTDataset(MedMNIST):\n",
        "    def __init__(self, split = 'train', augmentation: bool = False):\n",
        "        ''' Dataset class for Pneumonia MNST.\n",
        "        The provided init function will automatically download the necessary\n",
        "        files at the first class initialistion.\n",
        "\n",
        "        :param split: 'train', 'val' or 'test', select subset\n",
        "\n",
        "        '''\n",
        "        self.flag = \"pneumoniamnist\"\n",
        "        self.size = 28\n",
        "        self.size_flag = \"\"\n",
        "        self.root = './data/coursework/'\n",
        "        self.info = INFO[self.flag]\n",
        "        self.download()\n",
        "\n",
        "        npz_file = np.load(os.path.join(self.root, \"pneumoniamnist.npz\"))\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Load all the images\n",
        "        assert self.split in ['train','val','test']\n",
        "\n",
        "        self.imgs = npz_file[f'{self.split}_images']\n",
        "        self.labels = npz_file[f'{self.split}_labels']\n",
        "\n",
        "        self.do_augment = augmentation\n",
        "\n",
        "        # define the augmentation pipeline for classification\n",
        "        self.augmentation_pipeline = v2.Compose([\n",
        "            # I don't rotate because this introduces black artifacts into the image which may be misrepresented.\n",
        "            v2.CenterCrop((26,26)),\n",
        "            # We can flip because at this resolution there is no difference between the left and right lung\n",
        "            v2.RandomHorizontalFlip(),\n",
        "        ])\n",
        "\n",
        "        self.default_augmentation = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale=True)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TASK: Implement the __getitem__ function to return the image and its class label.\n",
        "        \n",
        "        # Get the image at the indexed position\n",
        "        x = np.expand_dims(self.imgs[index], axis=0)\n",
        "        x = torch.from_numpy(x)\n",
        "\n",
        "        if self.do_augment:\n",
        "            x = self.augmentation_pipeline(x)\n",
        "\n",
        "        return self.default_augmentation(x), self.labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVd7l4PiSOGQ"
      },
      "source": [
        "Again, we use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling your PneumoniaMNIST dataset. No changes needed for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_5yidccSOGQ"
      },
      "outputs": [],
      "source": [
        "class PneumoniaMNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, batch_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_set = PneumoniaMNISTDataset(split='train', augmentation=True)\n",
        "        self.val_set = PneumoniaMNISTDataset(split='val', augmentation=False)\n",
        "        self.test_set = PneumoniaMNISTDataset(split='test', augmentation=False)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Check** dataset implementation.\n",
        "\n",
        "Run the below cell to visualise a batch of your training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "# Initialise data module\n",
        "datamodule = PneumoniaMNISTDataModule()\n",
        "# Get train dataloader\n",
        "train_dataloader = datamodule.train_dataloader()\n",
        "# Get first batch\n",
        "batch = next(iter(train_dataloader))\n",
        "# Visualise the images\n",
        "images, labels = batch\n",
        "f, ax = plt.subplots(1, 8, figsize=(12,4))\n",
        "for i in range(8):\n",
        "  ax[i].imshow(images[i, 0], cmap='gray')\n",
        "  ax[i].set_title('label: ' + str(labels[i].item()))\n",
        "  ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU2tN86DChgL"
      },
      "source": [
        "### **Task C-2:** Implement a classification model with a linear layer attached to a pre-trained image encoder.\n",
        "\n",
        "We first download the weights of the two pre-trained image encoders. One of them has been trained with the self-supervised SimCLR objective on a large publicly available chest X-ray dataset (different from PneunomiaMNIST). The other encoder is a standard ImageNet backbone that has been trained with a supervised classification objective on the ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gX9NTQqSOGR"
      },
      "outputs": [],
      "source": [
        "# ! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/coursework.zip\n",
        "# ! unzip coursework.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KCcuk9ISOGR"
      },
      "source": [
        "We provide the function for loading the encoders. No changes needed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iHkCTqLETVr"
      },
      "outputs": [],
      "source": [
        "def load_encoder_from_checkpoint(checkpoint_path):\n",
        "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "  simclr_module = SimCLRModel()\n",
        "  print(simclr_module.load_state_dict(state_dict=ckpt))\n",
        "  return simclr_module.encoder.eval()\n",
        "\n",
        "imagenet_model = './data/coursework/model_imagenet.ckpt'\n",
        "chestxray_model = './data/coursework/model_chestxray.ckpt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otccmykgSOGS"
      },
      "source": [
        "Now, implement a classification model as a LightningModule for image classification using a pre-trained image encoder.\n",
        "\n",
        "The model should have a flag in the init function `freeze_encoder` that if set to true freezes all the weights in the encoder (used for linear probing), and if set to false all weights are trainable (used for model finetuning).\n",
        "\n",
        "*Hint:* Check out previous tutorials for inspiration on how to implement a classification model as LightningModule. For the coursework, we recommend using the Area Under the Receiver Operating Characteristic Curve (ROC-AUC) performance metric (instead of accuracy). ROC-AUC is measure of the overall discriminative power of a classification model. You can use the readily available implementation in [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/classification/auroc.html#functional-interface). You should log the ROC-AUC similar to how we logged accuracy in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_lHEp69DVqt"
      },
      "outputs": [],
      "source": [
        "# TASK: Implement the ImageClassifier class\n",
        "# Check previous tutorials for inspiration on how to implement an `ImageClassifier`\n",
        "\n",
        "class ImageClassifier(LightningModule):\n",
        "    def __init__(self, \n",
        "                 pretrained_encoder: torch.nn.Module, \n",
        "                 freeze_encoder: bool = True, \n",
        "                 output_dim: int = 2, \n",
        "                 learning_rate: float = 0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pretrained_encoder = pretrained_encoder\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # pre-trained image encoder\n",
        "        for param in self.pretrained_encoder.parameters():\n",
        "            param.requires_grad = not self.freeze_encoder\n",
        "\n",
        "        # classification linear layer\n",
        "        # in the paper, they implemented an MLP with one hidden layer z_i = g(h_i) = W_2*Ïƒ(W_1*h_i) where Ïƒ is a ReLU non-linearity\n",
        "        self.lin_layer = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pretrained_encoder(x)\n",
        "        x = self.lin_layer(x)\n",
        "        return x\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "    \n",
        "    def process_batch(self, batch):\n",
        "        x, y = batch\n",
        "        y = y.squeeze(dim=1)\n",
        "\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).float()\n",
        "        auroc_metric = auroc(preds, y, task='binary')\n",
        "\n",
        "        return loss, auroc_metric\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, acc = self.process_batch(batch)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, acc = self.process_batch(batch)\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', acc)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        # it is independent of forward\n",
        "        loss, auroc_metric = self.process_batch(batch)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', auroc_metric, prog_bar=True)\n",
        "        # if batch_idx == 0:\n",
        "        #     grid = torchvision.utils.make_grid(batch[0][0:16, ...], nrow=4, normalize=True)\n",
        "        #     self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTN7khmEFVba"
      },
      "source": [
        "### **Task C-3a:** Implement training and testing for linear probing.\n",
        "\n",
        "Train two classification models using **linear probing**, one for each of the two provided image encoders. Evaluate on both the validation and test sets.\n",
        "\n",
        "*Note:* Training for 25 epochs should be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjW7rGZFU1m"
      },
      "outputs": [],
      "source": [
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = PneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "# TASK: Implement the linear probing training and testing routines.\n",
        "\n",
        "classifier_imageNet_linprob = ImageClassifier(pretrained_encoder = load_encoder_from_checkpoint(imagenet_model), freeze_encoder=True)\n",
        "classifier_chestxray_linprob = ImageClassifier(pretrained_encoder= load_encoder_from_checkpoint(chestxray_model), freeze_encoder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Probing for classifier_imageNet_finetuned\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/classification/', name='classifier_imageNet_linprob'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "\n",
        "trainer.fit(model=classifier_imageNet_linprob, datamodule=data)\n",
        "\n",
        "print(\"Validation performance for ImageNet\")\n",
        "trainer.validate(model=classifier_imageNet_linprob, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "print(\"Test performance for ImageNet\")\n",
        "trainer.test(model=classifier_imageNet_linprob, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Probing for classifier_chestxray_finetuned\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/classification/', name='classifier_chestxray_linprob'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "\n",
        "trainer.fit(model=classifier_chestxray_linprob, datamodule=data)\n",
        "\n",
        "print(\"Validation performance for ChestXRay\")\n",
        "trainer.validate(model=classifier_chestxray_linprob, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "print(\"Test performance for ChestXRay\")\n",
        "trainer.test(model=classifier_chestxray_linprob, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDa5-HnyHoRh"
      },
      "source": [
        "### **Task C-3b:** Implement training and testing for model finetuning.\n",
        "\n",
        "Repeat the experiments, but this time using model finetuning instead of linear probing. Evaluate on both the validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF2RYgemHdf-"
      },
      "outputs": [],
      "source": [
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = PneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "classifier_imageNet_finetuned = ImageClassifier(pretrained_encoder = load_encoder_from_checkpoint(imagenet_model), freeze_encoder=False)\n",
        "classifier_chestxray_finetuned = ImageClassifier(pretrained_encoder= load_encoder_from_checkpoint(chestxray_model), freeze_encoder=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning for classifier_imageNet_finetuned\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/classification/', name='classifier_imageNet_finetuned'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "\n",
        "trainer.fit(model=classifier_imageNet_finetuned, datamodule=data)\n",
        "\n",
        "print(\"Validation performance for ImageNet\")\n",
        "trainer.validate(model=classifier_imageNet_finetuned, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "print(\"Test performance for ImageNet\")\n",
        "trainer.test(model=classifier_imageNet_finetuned, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning for classifier_chestxray_finetuned\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/classification/', name='classifier_chestXRay_finetuned'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'), TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "\n",
        "trainer.fit(model=classifier_chestxray_finetuned, datamodule=data)\n",
        "\n",
        "print(\"Validation performance for ImageNet\")\n",
        "trainer.validate(model=classifier_chestxray_finetuned, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "print(\"Test performance for ImageNet\")\n",
        "trainer.test(model=classifier_chestxray_finetuned, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9vluYnwSOGT"
      },
      "source": [
        "### **Task C-4:** Your evaluation report.\n",
        "\n",
        "Provide a brief summary (no more than 300 words) with an analysis of your findings. Try explaining the observed performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've re-ran the model since, but these were the numbers I was referring to (they're similar to the ones above)\n",
        "\n",
        "copying over the performance for convenience:\n",
        "- Linear Probing\n",
        "\t- imageNet\n",
        "\t\t- val_acc             0.883340060710907\n",
        "\t    - val_loss            0.20424185693264008\n",
        "\t    - test_acc\t          0.785427451133728\n",
        "        - test_loss           0.41231027245521545\n",
        "    - chestxray\n",
        "        - val_acc             0.8496582508087158\n",
        "\t    - val_loss            0.26481953263282776\n",
        "\t    - test_acc            0.7455279231071472\n",
        "        - test_loss           0.5827047228813171\n",
        "- FineTuning\n",
        "\t- imageNet\n",
        "\t\t- val_acc             0.9408426880836487\n",
        "        - val_loss            0.12713123857975006\n",
        "\t    - test_acc            0.745345950126648\n",
        "        - test_loss           0.7334343791007996\n",
        "    - chestxray\n",
        "        - val_acc             0.9651557207107544\n",
        "\t    - val_loss            0.05536622926592827\n",
        "        - test_acc            0.7902529239654541\n",
        "        - test_loss           0.948483407497406"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the fine tuning approach, layers are not frozen in the encoder. This allows for the gradient from learning the simple classifier head to trickle into the encoder. This allows for better performance when compared to the linear probing approach because the encoder may also learn how to complement with the decoder more. This is why we see an overall improvement in the validation accuracy between the two approaches. However, when we consider the test data, the results are very similar, only varying at most 5% (When compared to the validation accuracy whose accuracy varies by 10%).\n",
        "\n",
        "In the linear probing approach, we note that the layers have been frozen. This forces the simple decoder layer to learn more about the encoder's representation to provide the correct output. However, since this decoder layer is a very primitive linear layer, it cannot learn enough detail to fully learn the hidden representation. This is why we see that when compared to fine tuning the approach, the validation accuracy varies by approximately 10% in both cases. \n",
        "\n",
        "Other observations include the increase of test loss when fine tuning when compared to the linear probing. This is perhaps due to there being more trainable layers present in the fine tuning approach, which may explain the bigger number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- *Placeholder text with 300 words:* -->\n",
        "\n",
        "<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed id enim ut nibh pretium egestas. Maecenas at felis vel justo viverra ornare. Ut elit erat, sagittis in augue vel, ultrices pulvinar orci. Mauris posuere consequat lorem, non sodales purus convallis sed. Fusce ut purus eu risus pellentesque imperdiet quis a felis. Ut neque mi, porta vulputate metus in, ultrices consectetur mi. Ut eu fringilla turpis, vel accumsan felis. -->\n",
        "\n",
        "<!-- Morbi maximus, nunc quis accumsan euismod, sem urna eleifend lorem, non consequat mauris diam eget elit. Praesent at massa ante. Nulla semper, turpis eget elementum iaculis, felis ex ultricies mauris, ac lobortis odio dui ut massa. Morbi luctus varius turpis, sed maximus erat ullamcorper nec. Etiam quis magna in neque commodo pretium ac nec lacus. Etiam ornare euismod pharetra. Etiam pellentesque est at lectus iaculis, et semper tellus cursus. Integer blandit tellus lacus, ac cursus nisi aliquam quis. Donec ut aliquam erat. Duis at quam at velit malesuada sagittis. Aliquam vestibulum tellus vitae nibh finibus ullamcorper. Lorem ipsum dolor sit amet, consectetur adipiscing elit. In pretium eros et est auctor ultricies. Curabitur lectus velit, vulputate blandit turpis eget, maximus interdum nunc. -->\n",
        "\n",
        "<!-- Sed tincidunt scelerisque magna ut congue. Nulla porttitor a ante ac pellentesque. Nam porttitor velit vehicula, euismod ligula vitae, sagittis tortor. Mauris ut cursus augue. Morbi nec rhoncus sem. Fusce id dolor in elit faucibus laoreet. Quisque tempus odio non nisl elementum volutpat. Donec facilisis, urna sit amet congue pretium, metus ex vulputate magna, ut vulputate quam erat eu est. Aliquam diam magna, volutpat porta orci posuere, varius ornare odio. Nullam interdum faucibus sodales. Maecenas hendrerit sollicitudin erat, ut vestibulum neque interdum ac. Sed molestie suscipit rutrum. Aenean vitae ante purus. Sed sed maximus nibh. Nulla facilisi. Sed sit amet turpis vitae libero sagittis molestie a in ante. Sed ultricies interdum. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir './lightning_logs/coursework/'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
